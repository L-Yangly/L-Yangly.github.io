# ImageNet Classification with Deep Convolutional Neural Networks

摘要：AlexNet是首个图像分类中应用深层卷积网络达到图像分类较好效果，然后后续在imageNet中表现较好的算法都是在此基础上进化发展过来的。
<!--more-->

## 文献信息

| 信息 | 内容                                                         |
| ---- | ------------------------------------------------------------ |
| 日期 | 2001.01                                                      |
| 作者 | Alex Krizhevsky et al. ({akrizhevsky, geoffhinton}@google.com) |
| 机构 | Google Inc.                                                  |
| 来源 | [NIPS'12: Proceedings of the 25th International Conference on Neural Information Processing Systems](https://dl.acm.org/doi/proceedings/10.5555/2999134) |
| 链接 | [View Profile](https://dl.acm.org/profile/81488667660)[ImageNet Classification with Deep Convolutional Neural Networks](https://dl.acm.org/doi/10.5555/2999134.2999257) |
| 代码 | [Code]()                                                     |

## 个人理解
><strong style="color:red;">问题:</strong> 特点为通过数据驱动让模型自动学习特征，省去了人工寻找特征的步骤。但不同的模型也找出不同质量的特征，特征的质量直接影响到分类结果的准确度，表达能力更强的特征也给模型带来更强的分类能力。因此，深度网络通过数据学习到表达能力更强的特征。
> 
><strong style="color:red;">方法:</strong> 各种trick集成，ReLU、LRN、Overlapping Pooling、DropOut、双GPU和数据增强。
> 
><strong style="color:red;">结论:</strong> ImageNet 2012 图像分类冠军。
> 
><strong style="color:red;">理解:</strong> 各种Trick集成，从方方面面优化精度和效率。
> 
><strong style="color:red;">优化：</strong>局部响应归一化，全连接等需要优化。
---

## 背景知识

## 原理方法

### 1、ReLU Nonlinearity

一般神经元的激活函数会选择sigmoid函数或者tanh函数，然而Alex发现在训练时间的梯度衰减方面，这些非线性饱和函数要比非线性非饱和函数慢很多。

在AlexNet中用的非线性非饱和函数是$f=max(0,x)$，即ReLU。实验结果表明，要将深度网络训练至training error rate达到25%的话，ReLU只需5个epochs的迭代，但tanh单元需要35个epochs的迭代，用ReLU比tanh快6倍。论文中验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。

### 2、双GPU并行运行

为提高运行速度和提高网络运行规模，作者采用双GPU的设计模式。并且规定GPU只能在特定的层进行通信交流。其实就是每一个GPU负责一半的运算处理。作者的实验数据表示，two-GPU方案会比只用one-GPU跑半个上面大小网络的方案，在准确度上提高了1.7%的top-1和1.2%的top-5。值得注意的是，虽然one-GPU网络规模只有two-GPU的一半，但其实这两个网络其实并非等价的。

**本质上类似分组卷积，实现通道上的分离计算，提高计算效率。**

### 3、LRN局部响应归一化

LRN层：对局部神经元的活动创建竞争机制，使得响应较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。

这种归一化操作实现了某种形式的横向抑制，这也是受真实神经元的某种行为启发。因为卷积核矩阵的排序是随机任意，并且在训练之前就已经决定好顺序，这种LPN形成了一种横向抑制机制。

ReLU本来是不需要对输入进行标准化，但本文发现进行局部标准化能提高性能。
$$
b_{x,y} ^ i = \frac{a_{x,y}^i}{(k + \alpha \sum_{j=max(0, i-n/2)} ^{min(N-1, i+n/2)} (a_{x,y}^i)^2)^\beta}
$$

其中a代表在特征图中第i个卷积核(x,y)坐标经过了ReLU激活函数的输出，n表示相邻的几个卷积核。N表示这一层总的卷积核数量。k, n, α和β是超参，他们的值是在验证集上实验得到的，其中k = 2，n = 5，α = 0.0001，β = 0.75。

但后续在VGG-16相关论文中已经指出LRN操作没有明显的效果，根据AlexNet文献中的表述，他[^01]**认为**LRN作用的领域是临近有影响关系的特征层，而AlexNet采用的却是随机选取特征层进行LRN操作，同时卷积提取特征后，相应的特征空间内各个特征通道并没有明显的关联性，因此其在图像识别中表现效果不佳，没有可应用的空间，后被弃用。

### 3、Overlapping Pooling重叠最大池化

最大池化的池化操作是通过n×n的窗口移动n格，实现最大值映射。

重叠池化则是通过n×n的窗口移动k格（k设定＜n），进行相应规则的映射。

重叠池化的方法可以限制特征层经过处理后过度泛化，该泛化能力过高将导致无法区分一些本应能够区分的数据，即弱化了边界能力。目前，主流神经网络模型中并不采用重叠池化的方法，因为如果出现过度泛化，减少maxpooling层即可达到，如果单一应用maxpooling并不至于造成过度泛化。

### 4、 Dropout

DropOut：用于随机丢弃建立好的模型中一些神经元节点的数量。

**本质：**

- 模型集成：如果从模型的角度分析，Dropout的功能本质上是实现了模型集成的作用，实现了某层神经元的指数级替换，即该层出现无数种可能。
- 遗传配对：DropOut的本质与自然界的基因配对遗传学是相适应的，每个子代的出现都是父代基因和母代基因的配对，如此会有无限种可能，然后在子代中优秀的个体经过自然选择得以留存。同样的，随机选择神经元也将出现多神经元的协同，进而选出较好的配对，并将该配对对应的权重有效保留下来。
- 数据增强：DropOut还附赠了数据增强效果，因为部分特征值因为dropout方式丢弃，从而创造了更多样式的特征空间，其对应的原始数据（假设对应，实际没有）则更多样。

### 4、AlexNet结构

<div align=center>
    <img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213812.png width=75% />
</div>


```
- AlexNet为8层结构，其中前5层为卷积层，后面3层为全连接层；学习参数有6千万个，神经元有650,000个
- AlexNet在两个GPU上运行；
- AlexNet在第2,4,5层均是前一层自己GPU内连接，第3层是与前面两层全连接，全连接是2个GPU全连接；
- RPN层第1,2个卷积层后；
- Max pooling层在RPN层以及第5个卷积层后。
- ReLU在每个卷积层以及全连接层后。
- 卷积核大小数量：
  - conv1:96 11x11x3(个数/长/宽/深度)
  - conv2:256 5x5x48
  - conv3:384 3x3x256
  - conv4: 384 3x3x192
  - conv5: 256 3x3x192
  
ReLU、双GPU运算：提高训练速度。（应用于所有卷积层和全连接层）
重叠pool池化层：提高精度，不容易产生过度拟合。（应用在第一层，第二层，第五层后面）
局部响应归一化层(LRN)：提高精度。（应用在第一层和第二层后面）
Dropout：减少过度拟合。（应用在前两个全连接层）
```

## 训练测试

### 1、数据增强

利用数据增强方法增加了数据集样本的泛化，防止过拟合，提高了模型训练结果的准确率。

1. 尺度处理：伸缩与插值处理，实现多尺度数据的拟合。

2. 窗口处理：通过随机裁剪、平移变换方式，不改变数据序列的情况下获取相似数据。

3. 噪声附着：由于现实工作中，随着采样参数的不同、噪声影响的不同等影响，采集的数据信息可能会存在一些差异，因此可以将一个信号附加不同的环境影响生成大量的可疑现实数据，以增强某数据泛化后的准确率。

4. PCA主成分处理：通过分析数据信号或RGB图片通道的主成分，实现对当前数据的主成分进行噪声附着，以使得其更贴近于真实数据或真实图片。

### 2、训练技巧细节

M-SGD，带momentum动量的随机梯度下降方法，引入动量是为了附着权重，防止在低效区域过久的消耗迭代次数，提高梯度反馈的变化程度。

> 动量v[+1] = 0.9v-0.0005·ε·w-ε·avg(∂L(w)/ ∂w)[batch]
> 权重W[+1] = v[+1]+w

- 0.0005·ε·w—weight decay,权重微调，正则化和降低模型误差效果
- avg(∂L(w)/ ∂w)[batch]—一批数据的随机梯度下降均值
- ε为设定的学习率,[]内元素可以看作下标
- 损失函数E=E0+λΣw2，多分类交叉熵损失+LRN正则化，降低影响权重，防止过拟合。

其最后一层为1000分类的softmax，其损失函数即交叉熵，等效于最大化对数似然概率sum[log(pi)],i=1->1000。W

## 参考文献
[^01]: [小蔡叔叔开方舟-深度解读与思考——AlexNet深层卷积网络-知乎](https://zhuanlan.zhihu.com/p/408783370)


