<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>MobileNets, Efficient Convolutional Neural Networks for MobileVision Applications - Yang</title><meta name=Description content><meta property="og:title" content="MobileNets, Efficient Convolutional Neural Networks for MobileVision Applications"><meta property="og:description" content="摘要：MobileNet V1是由google2016年提出，主要创新点在于深度可分离卷积。"><meta property="og:type" content="article"><meta property="og:url" content="https://Yangliuly1.github.io/201704_mobilenet-v1/"><meta property="og:image" content="https://Yangliuly1.github.io/images/avatar.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-29T20:00:49+08:00"><meta property="article:modified_time" content="2022-04-29T20:00:49+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Yangliuly1.github.io/images/avatar.png"><meta name=twitter:title content="MobileNets, Efficient Convolutional Neural Networks for MobileVision Applications"><meta name=twitter:description content="摘要：MobileNet V1是由google2016年提出，主要创新点在于深度可分离卷积。"><meta name=application-name content="Yang"><meta name=apple-mobile-web-app-title content="Yang"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://Yangliuly1.github.io/201704_mobilenet-v1/><link rel=prev href=https://Yangliuly1.github.io/201608_densnet/><link rel=next href=https://Yangliuly1.github.io/201801_mobilenet-v2/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"MobileNets, Efficient Convolutional Neural Networks for MobileVision Applications","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/Yangliuly1.github.io\/201704_mobilenet-v1\/"},"genre":"posts","keywords":"图像分类","wordcount":364,"url":"https:\/\/Yangliuly1.github.io\/201704_mobilenet-v1\/","datePublished":"2022-04-29T20:00:49+08:00","dateModified":"2022-04-29T20:00:49+08:00","publisher":{"@type":"Organization","name":"Yangliuly1"},"author":{"@type":"Person","name":"Yangliuly1"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark")}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i> </a><span class="menu-item delimiter"></span><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i></a><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1模型复杂度与硬件性能的衡量>1、模型复杂度与硬件性能的衡量</a><ul><li><a href=#模型复杂度的衡量>模型复杂度的衡量</a></li><li><a href=#硬件性能的衡量>硬件性能的衡量</a></li></ul></li><li><a href=#2模型复杂度的计算公式>2、模型复杂度的计算公式</a></li><li><a href=#3mobilenet>3、MobileNet</a></li><li><a href=#4网络结构>4、网络结构</a></li></ul></li><li><a href=#实验结果>实验结果</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">MobileNets, Efficient Convolutional Neural Networks for MobileVision Applications</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=/about/ title=Author rel=author class=author>Yangliuly1</a>
</span>&nbsp;<span class=post-category>included in </span>&nbsp;<span class=post-category>category <a href=/categories/%E4%B8%93%E4%B8%9A%E8%AE%BA%E6%96%87/><i class="far fa-folder fa-fw"></i>专业论文</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-04-29>2022-04-29</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2022-04-29>2022-04-29</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;364 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;2 minutes&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1模型复杂度与硬件性能的衡量>1、模型复杂度与硬件性能的衡量</a><ul><li><a href=#模型复杂度的衡量>模型复杂度的衡量</a></li><li><a href=#硬件性能的衡量>硬件性能的衡量</a></li></ul></li><li><a href=#2模型复杂度的计算公式>2、模型复杂度的计算公式</a></li><li><a href=#3mobilenet>3、MobileNet</a></li><li><a href=#4网络结构>4、网络结构</a></li></ul></li><li><a href=#实验结果>实验结果</a></li></ul></nav></div></div><div class=content id=content><p>摘要：MobileNet V1是由google2016年提出，主要创新点在于深度可分离卷积。</p><h2 id=文献信息 class=headerLink><a href=#%e6%96%87%e7%8c%ae%e4%bf%a1%e6%81%af class=header-mark></a>文献信息</h2><table><thead><tr><th>信息</th><th>内容</th></tr></thead><tbody><tr><td>日期</td><td>2017.04</td></tr><tr><td>作者</td><td>Andrew G. Howard et al.</td></tr><tr><td>机构</td><td>Google Inc</td></tr><tr><td>来源</td><td>arXiv.</td></tr><tr><td>链接</td><td><a href=https://arxiv.org/abs/1704.04861 target=_blank rel="noopener noreffer">MobileNets: Efficient Convolutional Neural Networks for MobileVision Applications</a></td></tr><tr><td>代码</td><td><a href rel>Code</a></td></tr></tbody></table><h2 id=个人理解 class=headerLink><a href=#%e4%b8%aa%e4%ba%ba%e7%90%86%e8%a7%a3 class=header-mark></a>个人理解</h2><blockquote><p><strong style=color:red>问题:</strong> 从计算量角度考虑模型优化；</p><p><strong style=color:red>方法:</strong> 深度可分离卷积+宽度和分辨率超参；</p><p><strong style=color:red>结论:</strong> 在资源和精度权衡方面进行了大量实验，与其他流行的ImageNet分类模型相比，我们表现出了强大的性能。然后，展示了MobileNet在广泛的应用和用例中的有效性，包括目标检测、细粒度分类、人脸属性和大规模地理定位；</p><p><strong style=color:red>理解:</strong></p><ol><li>深度可分离卷积；</li><li>点卷积；</li><li>宽度和分辨率超参；</li></ol><p><strong style=color:red>优化：</strong>无。</p></blockquote><hr><h2 id=背景知识 class=headerLink><a href=#%e8%83%8c%e6%99%af%e7%9f%a5%e8%af%86 class=header-mark></a>背景知识</h2><ol><li>为了追求分类准确度，模型深度越来越深，模型复杂度也越来越高。模型过于庞大，面临着内存不足的问题，其次这些场景要求低延迟，或者说响应速度要快。</li><li>在某些真实的应用场景如移动或者嵌入式设备，如此大而复杂的模型是难以被应用的。</li><li>一是对训练好的复杂模型进行压缩得到小模型；二是直接设计小模型并进行训练。不管如何，其目标在保持模型性能（accuracy）的前提下降低模型大小（parameters size），同时提升模型速度（speed, low latency）。</li></ol><h2 id=原理方法 class=headerLink><a href=#%e5%8e%9f%e7%90%86%e6%96%b9%e6%b3%95 class=header-mark></a>原理方法</h2><h3 id=1模型复杂度与硬件性能的衡量 class=headerLink><a href=#1%e6%a8%a1%e5%9e%8b%e5%a4%8d%e6%9d%82%e5%ba%a6%e4%b8%8e%e7%a1%ac%e4%bb%b6%e6%80%a7%e8%83%bd%e7%9a%84%e8%a1%a1%e9%87%8f class=header-mark></a>1、模型复杂度与硬件性能的衡量</h3><h4 id=模型复杂度的衡量 class=headerLink><a href=#%e6%a8%a1%e5%9e%8b%e5%a4%8d%e6%9d%82%e5%ba%a6%e7%9a%84%e8%a1%a1%e9%87%8f class=header-mark></a>模型复杂度的衡量</h4><p>参数数量（Params）：指模型含有多少参数，直接决定模型的大小，也影响推断时对内存的占用量。</p><ul><li>单位通常为 M，通常参数用 float32 表示，所以模型大小是参数数量的 4 倍左右</li><li>参数数量与模型大小转换示例：$10M \ \ float32 \ \ bit = 10M \times 4 Byte = 40MB$</li></ul><p>理论计算量（FLOPs）：指模型推断时需要多少计算次数。</p><ul><li>是 floating point operations 的缩写（注意 s 小写），可以用来衡量算法/模型的复杂度，这关系到算法速度，大模型的单位通常为 G（GFLOPs：10亿次浮点运算），小模型单位通常为 M。</li><li>通常只考虑乘加操作(Multi-Adds)的数量，而且只考虑 CONV 和 FC 等参数层的计算量，忽略 BN 和 PReLU 等等。一般情况，CONV 和 FC 层也会 忽略仅纯加操作 的计算量，如 bias 偏置加和 shotcut 残差加等，目前有 BN 的卷积层可以不加 bias。</li><li>PS：也有用 MAC（Memory Access Cost） 表示的。</li></ul><h4 id=硬件性能的衡量 class=headerLink><a href=#%e7%a1%ac%e4%bb%b6%e6%80%a7%e8%83%bd%e7%9a%84%e8%a1%a1%e9%87%8f class=header-mark></a>硬件性能的衡量</h4><ul><li>算力： 计算平台倾尽全力每秒钟所能完成的浮点运算数（计算速度），单位一般为 TFLOPS（floating point of per second）。</li><li>带宽： 计算平台倾尽全力每秒所能完成的内存（CPU 内存 or GPU 显存）交换量，单位一般为 GB/s（GByte/second），计算公式一般为 内存频率 × 内存位宽 / 8。</li></ul><h3 id=2模型复杂度的计算公式 class=headerLink><a href=#2%e6%a8%a1%e5%9e%8b%e5%a4%8d%e6%9d%82%e5%ba%a6%e7%9a%84%e8%ae%a1%e7%ae%97%e5%85%ac%e5%bc%8f class=header-mark></a>2、模型复杂度的计算公式</h3><p>假设卷积核大小为$K_h \times K_w$，输入通道数为$C_{in}$，输出通道数为$C_{out}$，输出特征图的宽和高分别为W和H，忽略偏执项。</p><ul><li>Conv标准卷积层：<ul><li>Parmas:$C_{in} \times K_h \times K_w \times C_{out}$</li><li><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213851.png width=75%>
<img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213852.png width=75%>
<img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213852.png width=75%></div></li></ul></li><li>FC全连接层（相当于k=1）<ul><li>Parmas:$C_{in} \times C_{out}$</li><li>FLOPs: $C_{in} \times C_{out}$</li></ul></li><li>参数量与计算量: <code>https://github.com/Lyken17/pytorch-OpCounter</code>。</li><li>Group Conv：<ul><li>Parmas:$(K^2 \times \frac{C_{in}}{g} \times \frac{C_{out}}{g}) \times g = k^2 C_{in} C_{out}/g$</li><li>FLOPs: $k^2 C_{in} C_{out}/g \times W \times H$</li></ul></li><li>Depthwise Conv (DWConv)：即特殊分组$g=C_{in}=C_{out}$<ul><li>Parmas:$(K^2 \times \frac{C_{in}}{g} \times \frac{C_{out}}{g}) \times g = k^2 C_{in}$</li><li>FLOPs: $k^2 C_{in} \times W \times H$</li></ul></li></ul><h3 id=3mobilenet class=headerLink><a href=#3mobilenet class=header-mark></a>3、MobileNet</h3><ol><li>Depthwise separable convolution</li></ol><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213853.png width=50%>
<img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213854.png width=50%></div><p>深度级可分离卷积其实是一种可分解卷积操作（factorized convolutions），其可以分解为两个更小的操作：depthwise convolution和pointwise convolution，分别起到滤波和线性组合的作用，同时减少参数量和计算量。</p><ul><li><strong>TODO:如何分离的？</strong></li></ul><p>Depthwise convolution和标准卷积不同，对于标准卷积其卷积核是用在所有的输入通道上（input channels），而depthwise convolution针对每个输入通道采用不同的卷积核，就是说一个卷积核对应一个输入通道，所以说depthwise convolution是depth级别的操作。</p><p>而pointwise convolution其实就是普通的卷积，只不过其采用1x1的卷积核。</p><p>参数量和计算量：</p><p>$$
#params=k^2 c_{in} + c_{in}c_{out} \
#MultiAdd=k^2 c_{in} \times h_{out} w_{out} + 1 \times 1 \times c_{in}c_{out} \times h_{out} w_{out}
$$
相比于标准卷积，理论上的加速比例可达：
$$
\frac{k^2 c_{in} \times h_{out} w_{out} + 1 \times 1 \times c_{in}c_{out} \times h_{out} w_{out}}{k^2 c_{in} c_{out} \times h_{out} w_{out}} = \frac{1}{c_{out}} + \frac{1}{k^2}
$$
若k=3，参数量大约会减少到原来的 1/8 → 1/9。</p><p>Note：原论文中对第一层没有用此卷积，深度可分离卷积中的每一个后面都跟 BN 和 RELU。</p><p>Note：采用 depth-wise convolution 会有一个问题，就是导致信息流通不畅，即输出的 feature map 仅包含输入的 feature map 的一部分，而MobileNet 采用了 point-wise(1*1) convolution 帮助信息在通道之间流通。</p><ol start=2><li>Pooling layer</li></ol><p>Global Average Pooling：这一层没有参数，计算量可以忽略不计。</p><p>CONV/s2（步进2的卷积）代替 MaxPool+CONV：使得参数数量不变，计算量变为原来的 1/4 左右，且省去了MaxPool 的计算量。</p><ol start=3><li>两个超参数Width Multiplier和Resolution Multiplier</li></ol><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213854.png width=75%></div><ul><li>Width Multiplier($\alpha$): Thinner Models， 目的是使模型变瘦。<ul><li>所有层的通道数（channel） 乘以$\alpha$ 参数(四舍五入)，模型大小近似下降到原来的$\alpha^{2}$倍，计算量下降到原来的 $\alpha^{2}$倍</li><li>$\alpha \in (0, 1]$ with typical settings of 1, 0.75, 0.5 and 0.25，降低模型的宽度。</li></ul></li><li>Resolution Multiplier($\rho$): Reduced Representation，目的是降低图片的分辨率。<ul><li>输入层的分辨率（resolution）乘以$\rho$参数(四舍五入)，等价于所有层的分辨率乘$\rho$，模型大小不变，计算量下降到原来的 $\rho^{2}$倍。</li><li>$rho \in (0, 1]$降低输入图像的分辨率，一般输入图片的分辨率是224, 192, 160 or 128。</li></ul></li></ul><p>计算量：
$$
#MultiAdd=k^2 \times \alpha c_{in} \times \alpha c_{out} \times \rho h_{out} \times \rho w_{out} \ + 1 \times 1 \times \alpha c_{in} \times \alpha c_{out} \times \rho h_{out} \times \rho w_{out}
$$</p><h3 id=4网络结构 class=headerLink><a href=#4%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84 class=header-mark></a>4、网络结构</h3><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213855.png width=75%></div><h2 id=实验结果 class=headerLink><a href=#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c class=header-mark></a>实验结果</h2><p>实验结果如下，MobileNet采用k=3的卷积核，所以一般可达8-9倍加速，而精度不损失太多。更多有意思的细节和实验请参考原文。关于超参数的选择，准确度和参数量和参数运算量的关系，之间有个trade off，合理选择参数即可。</p><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213856.png width=75%>
<img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213856.png width=75%>
<img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213857.png width=75%>
<imghttps://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220429203829899.png src="width=75%"><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213858.png width=75%></div>## 参考文献
[^01]: [kai.han-轻量级CNN之MobileNet系列-知乎](https://zhuanlan.zhihu.com/p/45209964)</div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-04-29</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/>图像分类</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/201608_densnet/ class=prev rel=prev title="Densely Connected Convolutional Networks"><i class="fas fa-angle-left fa-fw"></i>Densely Connected Convolutional Networks</a>
<a href=/201801_mobilenet-v2/ class=next rel=next title="MobileNetV2, Inverted Residuals and Linear Bottlenecks">MobileNetV2, Inverted Residuals and Linear Bottlenecks<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/about/ target=_blank rel="noopener noreferrer">Yangliuly1</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},data:{"desktop-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!","mobile-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},table:{sort:!0},typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},duration:-1,speed:100}}</script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/typeit/typeit.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript></div></body></html>