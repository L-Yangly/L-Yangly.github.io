<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>ImageNet Classification with Deep Convolutional Neural Networks - Yang</title><meta name=Description content><meta property="og:title" content="ImageNet Classification with Deep Convolutional Neural Networks"><meta property="og:description" content="摘要：AlexNet是首个图像分类中应用深层卷积网络达到图像分类较好效果，然后后续在imageNet中表现较好的算法都是在此基础上进化发展过来的。"><meta property="og:type" content="article"><meta property="og:url" content="https://Yangliuly1.github.io/200101-alexnet/"><meta property="og:image" content="https://Yangliuly1.github.io/images/avatar.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-26T10:17:56+08:00"><meta property="article:modified_time" content="2022-04-26T10:17:56+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Yangliuly1.github.io/images/avatar.png"><meta name=twitter:title content="ImageNet Classification with Deep Convolutional Neural Networks"><meta name=twitter:description content="摘要：AlexNet是首个图像分类中应用深层卷积网络达到图像分类较好效果，然后后续在imageNet中表现较好的算法都是在此基础上进化发展过来的。"><meta name=application-name content="Yang"><meta name=apple-mobile-web-app-title content="Yang"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://Yangliuly1.github.io/200101-alexnet/><link rel=prev href=https://Yangliuly1.github.io/199811-lenet/><link rel=next href=https://Yangliuly1.github.io/201409-googlenet/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"ImageNet Classification with Deep Convolutional Neural Networks","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/Yangliuly1.github.io\/200101-alexnet\/"},"genre":"posts","keywords":"图像分类","wordcount":185,"url":"https:\/\/Yangliuly1.github.io\/200101-alexnet\/","datePublished":"2022-04-26T10:17:56+08:00","dateModified":"2022-04-26T10:17:56+08:00","publisher":{"@type":"Organization","name":"Yangliuly1"},"author":{"@type":"Person","name":"Yangliuly1"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark")}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i> </a><span class="menu-item delimiter"></span><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i></a><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1relu-nonlinearity>1、ReLU Nonlinearity</a></li><li><a href=#2双gpu并行运行>2、双GPU并行运行</a></li><li><a href=#3lrn局部响应归一化>3、LRN局部响应归一化</a></li><li><a href=#3overlapping-pooling重叠最大池化>3、Overlapping Pooling重叠最大池化</a></li><li><a href=#4-dropout>4、 Dropout</a></li><li><a href=#4alexnet结构>4、AlexNet结构</a></li></ul></li><li><a href=#训练测试>训练测试</a><ul><li><a href=#1数据增强>1、数据增强</a></li><li><a href=#2训练技巧细节>2、训练技巧细节</a></li></ul></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">ImageNet Classification with Deep Convolutional Neural Networks</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=/about/ title=Author rel=author class=author>Yangliuly1</a>
</span>&nbsp;<span class=post-category>included in </span>&nbsp;<span class=post-category>category <a href=/categories/%E4%B8%93%E4%B8%9A%E8%AE%BA%E6%96%87/><i class="far fa-folder fa-fw"></i>专业论文</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-04-26>2022-04-26</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2022-04-26>2022-04-26</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;185 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;One minute&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1relu-nonlinearity>1、ReLU Nonlinearity</a></li><li><a href=#2双gpu并行运行>2、双GPU并行运行</a></li><li><a href=#3lrn局部响应归一化>3、LRN局部响应归一化</a></li><li><a href=#3overlapping-pooling重叠最大池化>3、Overlapping Pooling重叠最大池化</a></li><li><a href=#4-dropout>4、 Dropout</a></li><li><a href=#4alexnet结构>4、AlexNet结构</a></li></ul></li><li><a href=#训练测试>训练测试</a><ul><li><a href=#1数据增强>1、数据增强</a></li><li><a href=#2训练技巧细节>2、训练技巧细节</a></li></ul></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></div><div class=content id=content><p>摘要：AlexNet是首个图像分类中应用深层卷积网络达到图像分类较好效果，然后后续在imageNet中表现较好的算法都是在此基础上进化发展过来的。</p><h1 id=imagenet-classification-with-deep-convolutional-neural-networks01 class=headerLink><a href=#imagenet-classification-with-deep-convolutional-neural-networks01 class=header-mark></a>ImageNet Classification with Deep Convolutional Neural Networks<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></h1><h2 id=文献信息 class=headerLink><a href=#%e6%96%87%e7%8c%ae%e4%bf%a1%e6%81%af class=header-mark></a>文献信息</h2><table><thead><tr><th>信息</th><th>内容</th></tr></thead><tbody><tr><td>日期</td><td>2001.01</td></tr><tr><td>作者</td><td>Alex Krizhevsky et al. ({akrizhevsky, <a href=mailto:geoffhinton%7d@google.com rel>geoffhinton}@google.com</a>)</td></tr><tr><td>机构</td><td>Google Inc.</td></tr><tr><td>来源</td><td><a href=https://dl.acm.org/doi/proceedings/10.5555/2999134 target=_blank rel="noopener noreffer">NIPS'12: Proceedings of the 25th International Conference on Neural Information Processing Systems</a></td></tr><tr><td>链接</td><td><a href=https://dl.acm.org/profile/81488667660 target=_blank rel="noopener noreffer">View Profile</a><a href=https://dl.acm.org/doi/10.5555/2999134.2999257 target=_blank rel="noopener noreffer">ImageNet Classification with Deep Convolutional Neural Networks</a></td></tr><tr><td>代码</td><td><a href rel>Code</a></td></tr></tbody></table><h2 id=个人理解 class=headerLink><a href=#%e4%b8%aa%e4%ba%ba%e7%90%86%e8%a7%a3 class=header-mark></a>个人理解</h2><blockquote><p><strong style=color:red>问题:</strong> 特点为通过数据驱动让模型自动学习特征，省去了人工寻找特征的步骤。但不同的模型也找出不同质量的特征，特征的质量直接影响到分类结果的准确度，表达能力更强的特征也给模型带来更强的分类能力。因此，深度网络通过数据学习到表达能力更强的特征。</p><p><strong style=color:red>方法:</strong> 各种trick集成，ReLU、LRN、Overlapping Pooling、DropOut、双GPU和数据增强。</p><p><strong style=color:red>结论:</strong> ImageNet 2012 图像分类冠军。</p><p><strong style=color:red>理解:</strong> 各种Trick集成，从方方面面优化精度和效率。</p><p><strong style=color:red>优化：</strong>局部响应归一化，全连接等需要优化。</p></blockquote><hr><h2 id=背景知识 class=headerLink><a href=#%e8%83%8c%e6%99%af%e7%9f%a5%e8%af%86 class=header-mark></a>背景知识</h2><h2 id=原理方法 class=headerLink><a href=#%e5%8e%9f%e7%90%86%e6%96%b9%e6%b3%95 class=header-mark></a>原理方法</h2><h3 id=1relu-nonlinearity class=headerLink><a href=#1relu-nonlinearity class=header-mark></a>1、ReLU Nonlinearity</h3><p>一般神经元的激活函数会选择sigmoid函数或者tanh函数，然而Alex发现在训练时间的梯度衰减方面，这些非线性饱和函数要比非线性非饱和函数慢很多。</p><p>在AlexNet中用的非线性非饱和函数是$f=max(0,x)$，即ReLU。实验结果表明，要将深度网络训练至training error rate达到25%的话，ReLU只需5个epochs的迭代，但tanh单元需要35个epochs的迭代，用ReLU比tanh快6倍。论文中验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。</p><h3 id=2双gpu并行运行 class=headerLink><a href=#2%e5%8f%8cgpu%e5%b9%b6%e8%a1%8c%e8%bf%90%e8%a1%8c class=header-mark></a>2、双GPU并行运行</h3><p>为提高运行速度和提高网络运行规模，作者采用双GPU的设计模式。并且规定GPU只能在特定的层进行通信交流。其实就是每一个GPU负责一半的运算处理。作者的实验数据表示，two-GPU方案会比只用one-GPU跑半个上面大小网络的方案，在准确度上提高了1.7%的top-1和1.2%的top-5。值得注意的是，虽然one-GPU网络规模只有two-GPU的一半，但其实这两个网络其实并非等价的。</p><p><strong>本质上类似分组卷积，实现通道上的分离计算，提高计算效率。</strong></p><h3 id=3lrn局部响应归一化 class=headerLink><a href=#3lrn%e5%b1%80%e9%83%a8%e5%93%8d%e5%ba%94%e5%bd%92%e4%b8%80%e5%8c%96 class=header-mark></a>3、LRN局部响应归一化</h3><p>LRN层：对局部神经元的活动创建竞争机制，使得响应较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</p><p>这种归一化操作实现了某种形式的横向抑制，这也是受真实神经元的某种行为启发。因为卷积核矩阵的排序是随机任意，并且在训练之前就已经决定好顺序，这种LPN形成了一种横向抑制机制。</p><p>ReLU本来是不需要对输入进行标准化，但本文发现进行局部标准化能提高性能。
$$
b_{x,y} ^ i = \frac{a_{x,y}^i}{(k + \alpha \sum_{j=max(0, i-n/2)} ^{min(N-1, i+n/2)} (a_{x,y}^i)^2)^\beta}
$$</p><p>其中a代表在特征图中第i个卷积核(x,y)坐标经过了ReLU激活函数的输出，n表示相邻的几个卷积核。N表示这一层总的卷积核数量。k, n, α和β是超参，他们的值是在验证集上实验得到的，其中k = 2，n = 5，α = 0.0001，β = 0.75。</p><p>但后续在VGG-16相关论文中已经指出LRN操作没有明显的效果，根据AlexNet文献中的表述，他<sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><strong>认为</strong>LRN作用的领域是临近有影响关系的特征层，而AlexNet采用的却是随机选取特征层进行LRN操作，同时卷积提取特征后，相应的特征空间内各个特征通道并没有明显的关联性，因此其在图像识别中表现效果不佳，没有可应用的空间，后被弃用。</p><h3 id=3overlapping-pooling重叠最大池化 class=headerLink><a href=#3overlapping-pooling%e9%87%8d%e5%8f%a0%e6%9c%80%e5%a4%a7%e6%b1%a0%e5%8c%96 class=header-mark></a>3、Overlapping Pooling重叠最大池化</h3><p>最大池化的池化操作是通过n×n的窗口移动n格，实现最大值映射。</p><p>重叠池化则是通过n×n的窗口移动k格（k设定＜n），进行相应规则的映射。</p><p>重叠池化的方法可以限制特征层经过处理后过度泛化，该泛化能力过高将导致无法区分一些本应能够区分的数据，即弱化了边界能力。目前，主流神经网络模型中并不采用重叠池化的方法，因为如果出现过度泛化，减少maxpooling层即可达到，如果单一应用maxpooling并不至于造成过度泛化。</p><h3 id=4-dropout class=headerLink><a href=#4-dropout class=header-mark></a>4、 Dropout</h3><p>DropOut：用于随机丢弃建立好的模型中一些神经元节点的数量。</p><p><strong>本质：</strong></p><ul><li>模型集成：如果从模型的角度分析，Dropout的功能本质上是实现了模型集成的作用，实现了某层神经元的指数级替换，即该层出现无数种可能。</li><li>遗传配对：DropOut的本质与自然界的基因配对遗传学是相适应的，每个子代的出现都是父代基因和母代基因的配对，如此会有无限种可能，然后在子代中优秀的个体经过自然选择得以留存。同样的，随机选择神经元也将出现多神经元的协同，进而选出较好的配对，并将该配对对应的权重有效保留下来。</li><li>数据增强：DropOut还附赠了数据增强效果，因为部分特征值因为dropout方式丢弃，从而创造了更多样式的特征空间，其对应的原始数据（假设对应，实际没有）则更多样。</li></ul><h3 id=4alexnet结构 class=headerLink><a href=#4alexnet%e7%bb%93%e6%9e%84 class=header-mark></a>4、AlexNet结构</h3><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220426223753898.png width=75%></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>- AlexNet为8层结构，其中前5层为卷积层，后面3层为全连接层；学习参数有6千万个，神经元有650,000个
</span></span><span class=line><span class=cl>- AlexNet在两个GPU上运行；
</span></span><span class=line><span class=cl>- AlexNet在第2,4,5层均是前一层自己GPU内连接，第3层是与前面两层全连接，全连接是2个GPU全连接；
</span></span><span class=line><span class=cl>- RPN层第1,2个卷积层后；
</span></span><span class=line><span class=cl>- Max pooling层在RPN层以及第5个卷积层后。
</span></span><span class=line><span class=cl>- ReLU在每个卷积层以及全连接层后。
</span></span><span class=line><span class=cl>- 卷积核大小数量：
</span></span><span class=line><span class=cl>  - conv1:96 11x11x3(个数/长/宽/深度)
</span></span><span class=line><span class=cl>  - conv2:256 5x5x48
</span></span><span class=line><span class=cl>  - conv3:384 3x3x256
</span></span><span class=line><span class=cl>  - conv4: 384 3x3x192
</span></span><span class=line><span class=cl>  - conv5: 256 3x3x192
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>ReLU、双GPU运算：提高训练速度。（应用于所有卷积层和全连接层）
</span></span><span class=line><span class=cl>重叠pool池化层：提高精度，不容易产生过度拟合。（应用在第一层，第二层，第五层后面）
</span></span><span class=line><span class=cl>局部响应归一化层(LRN)：提高精度。（应用在第一层和第二层后面）
</span></span><span class=line><span class=cl>Dropout：减少过度拟合。（应用在前两个全连接层）
</span></span></code></pre></td></tr></table></div></div><h2 id=训练测试 class=headerLink><a href=#%e8%ae%ad%e7%bb%83%e6%b5%8b%e8%af%95 class=header-mark></a>训练测试</h2><h3 id=1数据增强 class=headerLink><a href=#1%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%ba class=header-mark></a>1、数据增强</h3><p>利用数据增强方法增加了数据集样本的泛化，防止过拟合，提高了模型训练结果的准确率。</p><ol><li><p>尺度处理：伸缩与插值处理，实现多尺度数据的拟合。</p></li><li><p>窗口处理：通过随机裁剪、平移变换方式，不改变数据序列的情况下获取相似数据。</p></li><li><p>噪声附着：由于现实工作中，随着采样参数的不同、噪声影响的不同等影响，采集的数据信息可能会存在一些差异，因此可以将一个信号附加不同的环境影响生成大量的可疑现实数据，以增强某数据泛化后的准确率。</p></li><li><p>PCA主成分处理：通过分析数据信号或RGB图片通道的主成分，实现对当前数据的主成分进行噪声附着，以使得其更贴近于真实数据或真实图片。</p></li></ol><h3 id=2训练技巧细节 class=headerLink><a href=#2%e8%ae%ad%e7%bb%83%e6%8a%80%e5%b7%a7%e7%bb%86%e8%8a%82 class=header-mark></a>2、训练技巧细节</h3><p>M-SGD，带momentum动量的随机梯度下降方法，引入动量是为了附着权重，防止在低效区域过久的消耗迭代次数，提高梯度反馈的变化程度。</p><blockquote><p>动量v[+1] = 0.9v-0.0005·ε·w-ε·avg(∂L(w)/ ∂w)[batch]
权重W[+1] = v[+1]+w</p></blockquote><ul><li>0.0005·ε·w—weight decay,权重微调，正则化和降低模型误差效果</li><li>avg(∂L(w)/ ∂w)[batch]—一批数据的随机梯度下降均值</li><li>ε为设定的学习率,[]内元素可以看作下标</li><li>损失函数E=E0+λΣw2，多分类交叉熵损失+LRN正则化，降低影响权重，防止过拟合。</li></ul><p>其最后一层为1000分类的softmax，其损失函数即交叉熵，等效于最大化对数似然概率sum[log(pi)],i=1->1000。W</p><h2 id=参考文献 class=headerLink><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae class=header-mark></a>参考文献</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://zhuanlan.zhihu.com/p/408783370 target=_blank rel="noopener noreffer">小蔡叔叔开方舟-深度解读与思考——AlexNet深层卷积网络-知乎</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-04-26</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/>图像分类</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/199811-lenet/ class=prev rel=prev title="Gradient-Based Learning Applied to Document Recognition"><i class="fas fa-angle-left fa-fw"></i>Gradient-Based Learning Applied to Document Recognition</a>
<a href=/201409-googlenet/ class=next rel=next title="Going deeper with convolutions">Going deeper with convolutions<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/about/ target=_blank rel="noopener noreferrer">Yangliuly1</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},data:{"desktop-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!","mobile-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},table:{sort:!0},typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},duration:-1,speed:100}}</script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/typeit/typeit.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript></div></body></html>