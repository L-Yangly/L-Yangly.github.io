<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Yang</title><link>https://Yangliuly1.github.io/</link><description>Yang</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>yangliuly1993@gmail.com (Yangliuly1)</managingEditor><webMaster>yangliuly1993@gmail.com (Yangliuly1)</webMaster><lastBuildDate>Mon, 25 Apr 2022 14:01:24 +0800</lastBuildDate><atom:link href="https://Yangliuly1.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Gradient-Based Learning Applied to Document Recognition</title><link>https://Yangliuly1.github.io/199811-lenet/</link><pubDate>Mon, 25 Apr 2022 14:01:24 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/199811-lenet/</guid><description>&lt;p>摘要：LeNet-5 是一个专为手写数字识别而设计的最经典的卷积神经网络，被誉为早期卷积神经网络中最有代表性的实验系统之一。&lt;/p></description></item><item><title>Very Deep Convolutional Networks for Large-Scale Image Recognition</title><link>https://Yangliuly1.github.io/201409-vgg/</link><pubDate>Mon, 25 Apr 2022 13:22:58 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201409-vgg/</guid><description>&lt;p>摘要：VGG 小卷积替换大卷积，减少网络参数，提高网络深度。&lt;/p></description></item><item><title>Aggregated Residual Transformations for Deep Neural Networks</title><link>https://Yangliuly1.github.io/201611-resnext/</link><pubDate>Mon, 25 Apr 2022 09:24:21 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201611-resnext/</guid><description>&lt;p>摘要：ResNeXt，借鉴VGG、ResNet和Inception的思想组合一下，核心是分组卷积。&lt;/p></description></item><item><title>Identity Mappings in Deep Residual Networks</title><link>https://Yangliuly1.github.io/201603-resnet-v2/</link><pubDate>Sun, 24 Apr 2022 09:04:11 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201603-resnet-v2/</guid><description>&lt;p>摘要：ResNet v2 恒等连接的支路详细讨论与实验。&lt;/p></description></item><item><title>Deep Residual Learning for Image Recognition</title><link>https://Yangliuly1.github.io/201512-resnet-v1/</link><pubDate>Sat, 23 Apr 2022 11:59:33 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201512-resnet-v1/</guid><description>&lt;p>摘要：残差学习的目的是让模型的内部结构至少有恒等映射的能力，以保证在堆叠网络的过程中，网络至少不会因为继续堆叠而产生退化!&lt;/p></description></item></channel></rss>