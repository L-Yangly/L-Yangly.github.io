<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>Learning Transferable Architectures for Scalable Image Recognition - Yang</title><meta name=Description content><meta property="og:title" content="Learning Transferable Architectures for Scalable Image Recognition"><meta property="og:description" content="摘要：cvpr2017 google brain作品，利用强化学习，使用500块p100训练4天多得到的网络结构NASNet，在小数据（CIFAR-10）上学习一个网络单元（Cell），然后通过堆叠更多的这些网络单元的形式将网络迁移到更复杂，尺寸更大的数据集上面，不管在精度还是在速度上都超越了人工设计的经典结构。"><meta property="og:type" content="article"><meta property="og:url" content="https://Yangliuly1.github.io/201707-nasnet/"><meta property="og:image" content="https://Yangliuly1.github.io/images/avatar.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-07T23:18:52+08:00"><meta property="article:modified_time" content="2022-05-07T23:18:52+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Yangliuly1.github.io/images/avatar.png"><meta name=twitter:title content="Learning Transferable Architectures for Scalable Image Recognition"><meta name=twitter:description content="摘要：cvpr2017 google brain作品，利用强化学习，使用500块p100训练4天多得到的网络结构NASNet，在小数据（CIFAR-10）上学习一个网络单元（Cell），然后通过堆叠更多的这些网络单元的形式将网络迁移到更复杂，尺寸更大的数据集上面，不管在精度还是在速度上都超越了人工设计的经典结构。"><meta name=application-name content="Yang"><meta name=apple-mobile-web-app-title content="Yang"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://Yangliuly1.github.io/201707-nasnet/><link rel=prev href=https://Yangliuly1.github.io/201807-shufflenet-v2/><link rel=next href=https://Yangliuly1.github.io/201804-amoebanet/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Learning Transferable Architectures for Scalable Image Recognition","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/Yangliuly1.github.io\/201707-nasnet\/"},"genre":"posts","keywords":"图像分类","wordcount":455,"url":"https:\/\/Yangliuly1.github.io\/201707-nasnet\/","datePublished":"2022-05-07T23:18:52+08:00","dateModified":"2022-05-07T23:18:52+08:00","publisher":{"@type":"Organization","name":"Yangliuly1"},"author":{"@type":"Person","name":"Yangliuly1"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark")}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(t){const e=document.getElementsByTagName("meta");for(let n=0;n<e.length;n++)if(e[n].getAttribute("name")===t)return e[n];return''}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?setTheme("dark"):setTheme("light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?setTheme("dark"):setTheme("light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class="far fa-edit fa-fw"></i></span><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i> </a><span class="menu-item delimiter"></span><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class="far fa-edit fa-fw"></i></span><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i></a><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1nasnet-搜索过程>1、NASNet 搜索过程</a></li><li><a href=#2nasnet-搜索空间>2、NASNet 搜索空间</a></li><li><a href=#4nasnet-网络结构>4、NASNet 网络结构</a></li></ul></li><li><a href=#实验结果>实验结果</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Learning Transferable Architectures for Scalable Image Recognition</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=/about/ title=Author rel=author class=author>Yangliuly1</a>
</span>&nbsp;<span class=post-category>included in </span>&nbsp;<span class=post-category>category <a href=/categories/%E4%B8%93%E4%B8%9A%E8%AE%BA%E6%96%87/><i class="far fa-folder fa-fw"></i>专业论文</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-05-07>2022-05-07</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2022-05-07>2022-05-07</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;455 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;3 minutes&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1nasnet-搜索过程>1、NASNet 搜索过程</a></li><li><a href=#2nasnet-搜索空间>2、NASNet 搜索空间</a></li><li><a href=#4nasnet-网络结构>4、NASNet 网络结构</a></li></ul></li><li><a href=#实验结果>实验结果</a></li></ul></nav></div></div><div class=content id=content><p>摘要：cvpr2017 google brain作品，利用强化学习，使用500块p100训练4天多得到的网络结构NASNet，在小数据（CIFAR-10）上学习一个网络单元（Cell），然后通过堆叠更多的这些网络单元的形式将网络迁移到更复杂，尺寸更大的数据集上面，不管在精度还是在速度上都超越了人工设计的经典结构。</p><h1 id=learning-transferable-architectures-for-scalable-image-recognition0102 class=headerLink><a href=#learning-transferable-architectures-for-scalable-image-recognition0102 class=header-mark></a>Learning Transferable Architectures for Scalable Image Recognition<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></h1><h2 id=文献信息 class=headerLink><a href=#%e6%96%87%e7%8c%ae%e4%bf%a1%e6%81%af class=header-mark></a>文献信息</h2><table><thead><tr><th>信息</th><th>内容</th></tr></thead><tbody><tr><td>日期</td><td>2017.07</td></tr><tr><td>作者</td><td>Barret Zoph et al.</td></tr><tr><td>机构</td><td>Google Brain</td></tr><tr><td>来源</td><td>cvpr2018</td></tr><tr><td>链接</td><td><a href=https://arxiv.org/abs/1707.07012 target=_blank rel="noopener noreffer">Learning Transferable Architectures for Scalable Image Recognition</a></td></tr><tr><td>代码</td><td><a href rel>Code</a></td></tr></tbody></table><h2 id=个人理解 class=headerLink><a href=#%e4%b8%aa%e4%ba%ba%e7%90%86%e8%a7%a3 class=header-mark></a>个人理解</h2><blockquote><p><strong style=color:red>问题:</strong> 文章为了解决什么问题；</p><p><strong style=color:red>方法:</strong> 文章提出了什么方法和技术；</p><p><strong style=color:red>结论:</strong> 文章结论即 数据集 + 评价 指标；</p><p><strong style=color:red>理解:</strong> 论文读完的感受与体会，比如该方法借鉴了什么思想，方法是不是新颖，实验怎么做的，讨论的变量是什么，还有其他值得读的文献。</p><p><strong style=color:red>优化：</strong>还有什么值得改进与优化的。</p></blockquote><hr><h2 id=背景知识 class=headerLink><a href=#%e8%83%8c%e6%99%af%e7%9f%a5%e8%af%86 class=header-mark></a>背景知识</h2><p>经典CNN结构如VGGNet、ResNet、DenseNet、MobileNet等等都是人工设计的，人工难免无法做到最优，既然深度学习这么强大，能否让CNN自己去设计CNN?</p><p>Google Brain在ICLR 2017最早提出NAS：</p><blockquote><p>Zoph B, Le Q V. Neural architecture search with reinforcement learning[J]. arXiv preprint arXiv:1611.01578, 2016.</p></blockquote><p>CNN的结构可以由RNN产生，生成的CNN在目标数据集上训练收敛后会得到accuracy，以accuracy作为监督信号，用RL (Reinforcement Learning)训练RNN控制器，这个过程不断循环，就能生产越来越好的CNN结构。</p><p>CVPR2018上NASNet:</p><blockquote><p>Zoph B, Vasudevan V, Shlens J, et al. Learning transferable architectures for scalable image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 8697-8710.</p></blockquote><p>ECCV 2018的PNAS：</p><blockquote><p>Liu C, Zoph B, Neumann M, et al. Progressive neural architecture search[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 19-34.</p></blockquote><p>ENAS：</p><blockquote><p>Pham H, Guan M Y, Zoph B, et al. Efficient neural architecture search via parameter sharing[J]. arXiv preprint arXiv:1802.03268, 2018.</p></blockquote><p>AAAI 2019的AmeobaNet-A：</p><blockquote><p>Real E, Aggarwal A, Huang Y, et al. Regularized evolution for image classifier architecture search[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 33: 4780-4789.</p></blockquote><p>Hardware-aware efficient NAS:</p><p>既然搜索来的block或许轻量但并不高效，第二类方法固定block结构转而搜索结构超参数，目前主流方法以MobileNetV2作为起点搜索，block用MBConv并保持其结构不动，只搜索stage和block相关的超参数，其中stage相关的超参数包括每个stage的block num和channel num，block相关的超参数包括kernel size, expansion factor, se ratio等。</p><p>Hardware-aware efficient NAS又可以分两个小类：</p><ul><li>第一小类是RNN作为controller用RL训练的NAS，特点是搜索空间大但搜索时间很长，需要有强大计算资源支撑，代表模型MnasNet；</li><li>第二小类叫differentialble NAS，常用方法是训练一个SuperNet并从中pruning出小网络的方法，特征是搜索空间比较小但搜索速度非常快，搜索时间与MnasNet相比，ICLR 2019的ProxylessNAS快200倍，CVPR 2019的FBNet快420倍，Single-Path NAS快5000倍。</li></ul><p>搜索FPN结构，比如CVPR 2019的NAS-FPN：</p><blockquote><p>Ghiasi G, Lin T Y, Le Q V. Nas-fpn: Learning scalable feature pyramid architecture for object detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 7036-7045.</p></blockquote><p>搜索图像分类的augmentation，比如CVPR 2019的AutoAugment：</p><blockquote><p>Cubuk E D, Zoph B, Mane D, et al. AutoAugment: Learning Augmentation Strategies From Data[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 113-123.</p></blockquote><p>搜索目标检测的augmentation，Learning Data Augmentation Strategies：</p><blockquote><p>Zoph B, Cubuk E D, Ghiasi G, et al. Learning Data Augmentation Strategies for Object Detection[J]. arXiv preprint arXiv:1906.11172, 2019.</p></blockquote><p>搜索Deeplab中的ASPP，如NeurIPS 2018的DPC：</p><blockquote><p>Chen L C, Collins M, Zhu Y, et al. Searching for efficient multi-scale architectures for dense image prediction[C]//Advances in Neural Information Processing Systems. 2018: 8699-8710.</p></blockquote><p>将Cell-basd NAS和ASPP一起搜索，CVPR 2019的Auto-DeepLab：</p><blockquote><p>Liu C, Chen L C, Schroff F, et al. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 82-92.</p></blockquote><p>搜索激活函数，如Swish，搜出来的结构是$x \times \alpha \times sigmod(\beta x)$，用于近期的MobileNetV3：</p><blockquote><p>Ramachandran P, Zoph B, Le Q V. Searching for activation functions[J]. arXiv preprint arXiv:1710.05941, 2017.</p></blockquote><p>搜索优化方法，如Neural Optimizer Search，搜索出AddSign和PowerSign：</p><blockquote><p>Bello I, Zoph B, Vasudevan V, et al. Neural optimizer search with reinforcement learning[C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017: 459-468.</p></blockquote><h2 id=原理方法 class=headerLink><a href=#%e5%8e%9f%e7%90%86%e6%96%b9%e6%b3%95 class=header-mark></a>原理方法</h2><h3 id=1nasnet-搜索过程 class=headerLink><a href=#1nasnet-%e6%90%9c%e7%b4%a2%e8%bf%87%e7%a8%8b class=header-mark></a>1、NASNet 搜索过程</h3><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220507233511566.png width=75%></div><p>控制器RNN：从搜索空间中以概率p预测网络结构A。</p><p>worker单元：以结构A构建子网络，学习该网络直到收敛，并得到准确性R。</p><p>参数更新：最终将梯度p*R传递给RNN控制器进行梯度更新。</p><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220507233800251.png width=75%></div><p>控制器依次选择哪些Feature Map作为输入（灰色部分）以及使用哪些运算（黄色部分）来计算输入的Feature Map。其中，每种方法，每种操作都对应于一个softmax损失。这样重复B次，得到一个最终block模块。最终的损失函数就有5B个，实验中最优的B=5。</p><ol><li>从第$h_{i-1}$个Feature Map或者第 $h_{i}$个Feature Map或者已生成的网络块中选择一个Feature Map作为hidden layer A的输入，从上图中可以看到三种不同输入Feature Map的情况；</li><li>采用和1类似的方法为Hidden Layer B选择一个输入；</li><li>为1的Feature Map选择一个运算；</li><li>为2的Feature Map选择一个运算；</li><li>选择一个合并3，4得到的Feature Map的运算。</li></ol><h3 id=2nasnet-搜索空间 class=headerLink><a href=#2nasnet-%e6%90%9c%e7%b4%a2%e7%a9%ba%e9%97%b4 class=header-mark></a>2、NASNet 搜索空间</h3><p>15种操作：恒等连接，1x1（卷积），3x3（卷积、深度可分离卷积、空洞卷积、平均池化、最大池化）,1x3+3x1(卷积)，5x5（深度可分离卷积，最大池化），7x7（深度可分离卷积、最大池化），1x7+7x1(卷积)；</p><p>合并操作：（1）单位加；（2）拼接。</p><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220507233855766.png width=75%></div>### 3、NASNet 细节<ol><li>激活函数统一使用ReLU，实验结果表明ELU nonlinearity效果略优于ReLU；</li><li>全部使用Valid卷积，padding值由卷积核大小决定；</li><li>Reduction Cell的Feature Map的数量需要乘以2，Normal Cell数量不变。初始数量人为设定，一般来说数量越多，计算越慢，效果越好；</li><li>Normal Cell的重复次数（N)人为设定；</li><li>深度可分离卷积在深度卷积和单位卷积中间不使用BN或ReLU;</li><li>使用深度可分离卷积时，该算法执行两次；</li><li>所有卷积遵循ReLU->卷积->BN的计算顺序；</li><li>为了保持Feature Map的数量的一致性，必要的时候添加1x1卷积。</li></ol><h3 id=4nasnet-网络结构 class=headerLink><a href=#4nasnet-%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84 class=header-mark></a>4、NASNet 网络结构</h3><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220507235912753.png width=75%></div><p>（1）<em>Normal Cell</em>：输出Feature Map和输入Feature Map的尺寸相同；（2）<em>Reduction Cell</em>：输出Feature Map对输入Feature Map进行了一次降采样，在Reduction Cell中，对使用Input Feature作为输入的操作（卷积或者池化）会默认步长为2。</p><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220507235957692.png width=75%></div>### 5、Scheduled Drop Path 计划DropPath<p>在优化类似于Inception的多分支结构时，以一定概率随机丢弃掉部分分支是避免过拟合的一种非常有效的策略，例如DropPath。但是DropPath对NASNet不是非常有效。</p><p>在NASNet的Scheduled Drop Path中，丢弃的概率会随着训练时间的增加线性增加。这么做的动机很好理解：训练的次数越多，模型越容易过拟合，DropPath的避免过拟合的作用才能发挥的越有效。</p><h2 id=实验结果 class=headerLink><a href=#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c class=header-mark></a>实验结果</h2><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220508000440227.png width=75%>
<img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220508000344539.png width=75%>
<img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220508000459903.png width=75%>
<img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220508000514636.png width=75%>
<img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220508000523898.png width=75%>
<img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220508000536477.png width=75%></div>## 参考文献<div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://zhuanlan.zhihu.com/p/61149576 target=_blank rel="noopener noreffer">YaqiLYU-NAS之Google和CVPR2019相关论文-知乎</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://zhuanlan.zhihu.com/p/52616166 target=_blank rel="noopener noreffer">大师兄-NASNet详解-知乎</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-05-07</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/>图像分类</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/201807-shufflenet-v2/ class=prev rel=prev title="ShuffleNet V2, Practical Guidelines for Efficient CNN Architecture Design"><i class="fas fa-angle-left fa-fw"></i>ShuffleNet V2, Practical Guidelines for Efficient CNN Architecture Design</a>
<a href=/201804-amoebanet/ class=next rel=next title="Regularized Evolution for Image Classifier Architecture Search">Regularized Evolution for Image Classifier Architecture Search<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/about/ target=_blank rel="noopener noreferrer">Yangliuly1</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},data:{"desktop-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!","mobile-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},table:{sort:!0},typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},duration:-1,speed:100}}</script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/typeit/typeit.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript></div></body></html>