<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>图像分类 - Tag - Yang</title><link>https://Yangliuly1.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/</link><description>图像分类 - Tag - Yang</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>yangliuly1993@gmail.com (Yangliuly1)</managingEditor><webMaster>yangliuly1993@gmail.com (Yangliuly1)</webMaster><lastBuildDate>Sun, 08 May 2022 01:03:50 +0800</lastBuildDate><atom:link href="https://Yangliuly1.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/" rel="self" type="application/rss+xml"/><item><title>Regularized Evolution for Image Classifier Architecture Search</title><link>https://Yangliuly1.github.io/201804-amoebanet/</link><pubDate>Sun, 08 May 2022 01:03:50 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201804-amoebanet/</guid><description>&lt;p>摘要：AmoebaNet是通过遗传算法的进化策略（Evolution）实现的模型结构的学习过程。&lt;/p></description></item><item><title>Learning Transferable Architectures for Scalable Image Recognition</title><link>https://Yangliuly1.github.io/201707-nasnet/</link><pubDate>Sat, 07 May 2022 23:18:52 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201707-nasnet/</guid><description>&lt;p>摘要：cvpr2017 google brain作品，利用强化学习，使用500块p100训练4天多得到的网络结构NASNet，在小数据（CIFAR-10）上学习一个网络单元（Cell），然后通过堆叠更多的这些网络单元的形式将网络迁移到更复杂，尺寸更大的数据集上面，不管在精度还是在速度上都超越了人工设计的经典结构。&lt;/p></description></item><item><title>ShuffleNet V2, Practical Guidelines for Efficient CNN Architecture Design</title><link>https://Yangliuly1.github.io/201807-shufflenet-v2/</link><pubDate>Sat, 07 May 2022 15:09:04 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201807-shufflenet-v2/</guid><description>&lt;p>摘要：ShuffleNet v2中以内存访问代价（Memory Access Cost，MAC）和GPU并行性的方向分析网络效率。&lt;/p></description></item><item><title>ShuffleNet, An Extremely Efficient Convolutional Neural Network for Mobile Devices</title><link>https://Yangliuly1.github.io/201701-shufflenet-v1/</link><pubDate>Sat, 07 May 2022 11:09:59 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201701-shufflenet-v1/</guid><description>&lt;p>摘要：ShuffleNet 解决分组卷积的计算量和特征通信问题。&lt;/p></description></item><item><title>SqueezeNet, AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</title><link>https://Yangliuly1.github.io/201602-squeezenet/</link><pubDate>Sat, 07 May 2022 10:36:38 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201602-squeezenet/</guid><description>&lt;p>摘要：轻量级网络SquezeNet，亮点在于Fire结构，先压缩再扩展。&lt;/p></description></item><item><title>Rethinking Bottleneck Structure for Efficient Mobile Network Design</title><link>https://Yangliuly1.github.io/202007-mobilenext/</link><pubDate>Fri, 06 May 2022 17:32:41 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/202007-mobilenext/</guid><description>&lt;p>摘要：依图科技&amp;amp;新加坡国立大学颜水成团队提出的一种对标MobileNetV2的网络架构MobileNeXt。它针对MobileNetV2的核心模块逆残差模块存在的问题进行了深度分析，提出了一种新颖的SandGlass模块，并用于组建了该文的MobileNeXt架构，SandGlass是一种通用的模块，它可以轻易的嵌入到现有网络架构中并提升模型性能。&lt;/p></description></item><item><title>Searching for MobileNetV3</title><link>https://Yangliuly1.github.io/201905-mobilenet-v3/</link><pubDate>Fri, 29 Apr 2022 22:38:17 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201905-mobilenet-v3/</guid><description>&lt;p>摘要：文章的亮点在于&lt;strong>网络的设计利用了NAS（network architecture search）算法以及NetAdapt algorithm算法&lt;/strong>。&lt;/p></description></item><item><title>MobileNets, Efficient Convolutional Neural Networks for MobileVision Applications</title><link>https://Yangliuly1.github.io/201704-mobilenet-v1/</link><pubDate>Fri, 29 Apr 2022 20:00:49 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201704-mobilenet-v1/</guid><description>&lt;p>摘要：MobileNet V1是由google2016年提出，主要创新点在于深度可分离卷积。&lt;/p></description></item><item><title>Densely Connected Convolutional Networks</title><link>https://Yangliuly1.github.io/201608-densnet/</link><pubDate>Thu, 28 Apr 2022 23:25:46 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201608-densnet/</guid><description>&lt;p>摘要：DenseNet脱离了加深网络层数(ResNet)和加宽网络结构(Inception)来提升网络性能的定式思维，从特征的角度考虑，通过特征重用和旁路(Bypass)设置,既大幅度减少了网络的参数量，又在一定程度上缓解了gradient vanishing问题的产生。结合信息流和特征复用的假设，DenseNet当之无愧成为2017年计算机视觉顶会的年度最佳论文。&lt;/p></description></item><item><title>Xception, Deep Learning with Depthwise Separable Convolutions</title><link>https://Yangliuly1.github.io/201608-xception/</link><pubDate>Thu, 28 Apr 2022 21:57:27 +0800</pubDate><author><name>Yangliuly1</name></author><guid>https://Yangliuly1.github.io/201608-xception/</guid><description>&lt;p>摘要：基于Inception v3是假设出发，即解耦通道相关性和空间相关性，进行简化，推导出深度可分离卷积，构建Inception v4网络。&lt;/p></description></item></channel></rss>