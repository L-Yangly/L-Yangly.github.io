<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>Gradient-Based Learning Applied to Document Recognition - Yang</title><meta name=Description content><meta property="og:title" content="Gradient-Based Learning Applied to Document Recognition"><meta property="og:description" content="摘要：LeNet-5 是一个专为手写数字识别而设计的最经典的卷积神经网络，被誉为早期卷积神经网络中最有代表性的实验系统之一。"><meta property="og:type" content="article"><meta property="og:url" content="https://Yangliuly1.github.io/199811-lenet/"><meta property="og:image" content="https://Yangliuly1.github.io/images/avatar.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-25T14:01:24+08:00"><meta property="article:modified_time" content="2022-04-25T14:01:24+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Yangliuly1.github.io/images/avatar.png"><meta name=twitter:title content="Gradient-Based Learning Applied to Document Recognition"><meta name=twitter:description content="摘要：LeNet-5 是一个专为手写数字识别而设计的最经典的卷积神经网络，被誉为早期卷积神经网络中最有代表性的实验系统之一。"><meta name=application-name content="Yang"><meta name=apple-mobile-web-app-title content="Yang"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://Yangliuly1.github.io/199811-lenet/><link rel=prev href=https://Yangliuly1.github.io/201409-vgg/><link rel=next href=https://Yangliuly1.github.io/201409-googlenet/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Gradient-Based Learning Applied to Document Recognition","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/Yangliuly1.github.io\/199811-lenet\/"},"genre":"posts","keywords":"图像分类","wordcount":132,"url":"https:\/\/Yangliuly1.github.io\/199811-lenet\/","datePublished":"2022-04-25T14:01:24+08:00","dateModified":"2022-04-25T14:01:24+08:00","publisher":{"@type":"Organization","name":"Yangliuly1"},"author":{"@type":"Person","name":"Yangliuly1"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark")}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(t){const e=document.getElementsByTagName("meta");for(let n=0;n<e.length;n++)if(e[n].getAttribute("name")===t)return e[n];return''}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?setTheme("dark"):setTheme("light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?setTheme("dark"):setTheme("light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class="far fa-edit fa-fw"></i></span><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i> </a><span class="menu-item delimiter"></span><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class="far fa-edit fa-fw"></i></span><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i></a><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1卷积网络>1、卷积网络</a></li><li><a href=#2lenet5>2、LeNet5</a></li><li><a href=#3损失函数>3、损失函数</a></li></ul></li><li><a href=#训练测试>训练测试</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Gradient-Based Learning Applied to Document Recognition</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=/about/ title=Author rel=author class=author>Yangliuly1</a>
</span>&nbsp;<span class=post-category>included in </span>&nbsp;<span class=post-category>category <a href=/categories/%E4%B8%93%E4%B8%9A%E8%AE%BA%E6%96%87/><i class="far fa-folder fa-fw"></i>专业论文</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-04-25>2022-04-25</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2022-04-25>2022-04-25</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;132 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;One minute&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1卷积网络>1、卷积网络</a></li><li><a href=#2lenet5>2、LeNet5</a></li><li><a href=#3损失函数>3、损失函数</a></li></ul></li><li><a href=#训练测试>训练测试</a></li></ul></nav></div></div><div class=content id=content><p>摘要：LeNet-5 是一个专为手写数字识别而设计的最经典的卷积神经网络，被誉为早期卷积神经网络中最有代表性的实验系统之一。</p><h1 id=gradient-based-learning-applied-to-document-recognition class=headerLink><a href=#gradient-based-learning-applied-to-document-recognition class=header-mark></a>Gradient-Based Learning Applied to Document Recognition</h1><h2 id=文献信息 class=headerLink><a href=#%e6%96%87%e7%8c%ae%e4%bf%a1%e6%81%af class=header-mark></a>文献信息</h2><table><thead><tr><th>信息</th><th>内容</th></tr></thead><tbody><tr><td>日期</td><td>19998.11</td></tr><tr><td>作者</td><td>Yann LeCun et al. 2018年图灵奖获得者</td></tr><tr><td>机构</td><td>贝尔实验室</td></tr><tr><td>来源</td><td>Proceedings of the IEEE</td></tr><tr><td>链接</td><td><a href=https://ieeexplore.ieee.org/document/726791 target=_blank rel="noopener noreffer">Gradient-Based Learning Applied to Document Recognition</a></td></tr><tr><td>代码</td><td><a href rel>Code</a></td></tr></tbody></table><h2 id=个人理解 class=headerLink><a href=#%e4%b8%aa%e4%ba%ba%e7%90%86%e8%a7%a3 class=header-mark></a>个人理解</h2><blockquote><p><strong style=color:red>问题:</strong> 文章为了解决全连接网络的尺寸大、不具备平移不变性，忽略图像的拓扑结构；</p><p><strong style=color:red>方法:</strong> 文章提出了卷积神经网络，局部感受野、权值共享和下采样；</p><p><strong style=color:red>结论:</strong> 在MNIST数据集上， Lel、Jet-5 模型可以达到大约99.4%的准确率；</p><p><strong style=color:red>理解:</strong> 卷积神经网络的鼻祖，为后续深度学习打下了基础，重点是局部连接的思想贯穿全文，不仅仅卷积神经网络，还有不完全连接等操作。</p><p><strong style=color:red>优化：</strong>卷积神经网络的优化过程可以从AleNex，VGG, ResNet, Inception等等。</p></blockquote><hr><h2 id=背景知识 class=headerLink><a href=#%e8%83%8c%e6%99%af%e7%9f%a5%e8%af%86 class=header-mark></a>背景知识</h2><p>传统的机器学习方式，手工设计特征+分类器。全连接网络经过梯度下降训练的多层网络学习复杂高维非线性映射的能力，使它们明显适合于传统模式识别模型中的图像识别任务。</p><p><strong>全连接网络缺点</strong></p><ol><li><p>图像往往尺寸很大，如果把图像看成行向量作为输入层，即使第一层全连接选择尽量少的神经元，第一层的参数也很多，导致整体参数都很多，计算量很大，模型也只适用于小图像。</p></li><li><p>对于图像来说，不具有平移和局部失真的不变性（全连接网络每个神经元感受到的都是整幅图像，对平移，形变不具有不变性。只要对同一幅图像加入一些扰动，输出就会不同）。</p></li><li><p>完全忽略了输入的拓扑结构（将一幅图像转换为行向量，行向量特征的顺序其实是可以打乱的，但是应该所有图像转换为行向量打乱的方式要相同。在不改变神经元的输出的前提下，输入数据可以是任意的顺序），这样训练网络其实对结果没有影响。但是，对于图像来说，一个像素与其相邻的像素往往是具有空间上的相关性的，那么可以从这个空间相关性出发提取到图像的一些局部的特征信息（如角特征和边缘特征等，应该传统方法中有局部特征提取的算子）。显然提取图像局部特征是全连接网络做不到的。</p></li></ol><h2 id=原理方法 class=headerLink><a href=#%e5%8e%9f%e7%90%86%e6%96%b9%e6%b3%95 class=header-mark></a>原理方法</h2><p>LeNet-5 是一个专为手写数字识别而设计的最经典的卷积神经网络，被誉为早期卷积神经网络中最有代表性的实验系统之一。LeNet-5 模型由Yann LeCun 教授于 1998 年在其论文《Gradient-Based Learning Appliedto Document Recognition》中提出，这篇论文对于现代卷积神经网络的研究仍具有指导意义，可以说是CNN领域的第一篇经典之作。在MNIST数据集上， Lel、Jet-5 模型可以达到大约99.4%的准确率，基于此神经网络模型而设计出的手写数字识别系统在 20 世纪 90年代被广泛应用于美国的多家银行进行支票手写字识别。根据 Yann LeCun 教授公开发表的论文的内容，可知 LeNet-5 模型共有8层（包括输入层和输出层），下图展示了LeNet-5模型的整体框架结构。</p><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/v2-4dead40e80896b3f9bcda6024391e3ca_r.jpg width=75%></div><h3 id=1卷积网络 class=headerLink><a href=#1%e5%8d%b7%e7%a7%af%e7%bd%91%e7%bb%9c class=header-mark></a>1、卷积网络</h3><p>为了保证位移、尺度、畸变(shift, scale, distortion invariance)不变性，提出了卷积神经网络，其三大核心思想：</p><ol><li><p>稀疏连接或局部感受野：基于图像局部相关的原理，保留了图像局部结构，同时减少了网络的权值。</p></li><li><p>权值共享：基于图像局部相关的原理，同时减少网络的权值参数。</p></li><li><p>下采样：对平移和形变更加鲁棒，实现特征的不变性，同时起到了一定的降维的作用。。</p></li></ol><p>CNN相对于传统机器学习的不同：传统的机器学习需要手工设计特征；CNN是把卷积核作为特征提取算子来对图像提取特征，由于卷积核的参数是学习得到的，因此相当于卷积神经网络是自己训练得到了特征提取器，使用的是网络自己学习到的特征。</p><h3 id=2lenet5 class=headerLink><a href=#2lenet5 class=header-mark></a>2、LeNet5</h3><p>LeNet5由7层CNN（不包含输入层）组成，上图中输入的原始图像大小是32×32像素，卷积层用Ci表示，下采样层（pooling，池化）用Si表示，全连接层用Fi表示。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>C1层（卷积层5x5）：6@28×28 无
</span></span><span class=line><span class=cl>S2层（下采样层，池化层）：6@14×14 sigmod函数
</span></span><span class=line><span class=cl>C3层（卷积层5x5）：16@10×10 无 （不完全连接）
</span></span><span class=line><span class=cl>S4（下采样层，池化层）：16@5×5 sigmod函数
</span></span><span class=line><span class=cl>C5层（卷积层，实际为全连接层）：120 双曲正切函数 
</span></span><span class=line><span class=cl>F6层（全连接层）：84 双曲正切函数 
</span></span><span class=line><span class=cl>OUTPUT层（输出层）：10 径向基函数（RBF）
</span></span></code></pre></td></tr></table></div></div><p><strong>不完全连接</strong>：</p><p>目的：（1）使得网络的连接数保持在合理的范围内，应该就是减小运算量。（2）更重要的是，打破网络对称性，希望不同的feature map能够因为与前一层不同的feature map相连而学习到不同的特征。</p><p>原因：将全连接换成具有局部感受野的卷积，可以提高网络的性能。卷积的特点是相比全连接来说，卷积在Height和Width的维度上，神经元的连接是稀疏的（下一层神经元只与上一层的感受野内的神经元相连，而不是和所有的神经元相连）。LeNet在channel维度上也引入这种稀疏连接的形式呢，下一层的神经元在channel维度上，不是与上一层的所有channel的神经元相连的，而是只与部分channel的神经元相连。</p><p>问题：将全连接换卷积的优势是，卷积在进行运算时，感受野内的像素由于在空间上是相邻的，其灰度具有高度的相关性，因此更有利于提取特征。但是在channel的维度上，这可就不一定了。相邻的channel之间应该是没什么关联的，所以如果在channel层面上也采用稀疏连接，到底应该选那几个channel呢？文章的作者应该也是不太清楚这一点，所以在设计网络的时候是先选相邻的三层，然后相邻的四层，然后不完全相邻的四层，最后是五层全连。</p><p>但是目前来看，LeNet的这种设计是被现代CNN抛弃了，原因可能是有两点：（1）很可能就是GoogLeNet中提到的，这种设计不利于硬件设备的运算，反而效率更低；（2）不知道哪几个channel的特征是相关性更强的，还需要计算不同层特征的相关性（可能就是风格转换中的Gram矩阵），很麻烦。</p><p>**径向基神经网络：**基于距离进行衡量两个数据的相近程度的，RBF网最显著的特点是隐节点采用输人模式与中心向量的距离（如欧氏距离）作为函数的自变量，并使用径向基函数（如函数）作为激活函数。径向基函数关于N维空间的一个中心点具有径向对称性，而且神经元的输人离该中心点越远，神经元的激活程度就越低。</p><h3 id=3损失函数 class=headerLink><a href=#3%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 class=header-mark></a>3、损失函数</h3><p>基于最大似然估计准则，在LeNet论文的情形中应该是最小均方误差(MSE)：</p><p>$$
E(W)=\frac{1}{P} \sum_{p=1}^{P} y_{D^{p}}\left(Z^{p}, W\right)
$$
其中，$y_{D^{p}}$表示第$D_{i}-th$的RBF输出
论文中提到了这个loss缺失了三个很重要的属性，但是作者通过在后面加了一项，对loss做了一个改进：
$$
E(W)=\frac{1}{P} \sum_{p=1}^{P}\left(y_{D^{p}}\left(Z^{p}, W\right)+\log \left(e^{-j}+\sum_{i} e^{-y_{i}\left(Z^{p}, W\right)}\right)\right)
$$
第二项的负数起到竞争的作用，它必然小于或等于第一项，因此这个损失函数是正的，常数j是正的，并且防止了准备好的非常大的类的惩罚被进一步推高。这种判别准则防止了在学习RBF参数时出现上述折叠现象，因为它使RBF中心彼此分离。</p><h2 id=训练测试 class=headerLink><a href=#%e8%ae%ad%e7%bb%83%e6%b5%8b%e8%af%95 class=header-mark></a>训练测试</h2><p>注意：在论文里说输入像素的值背景层(白色)的corresp值为-0.1，前景层(黑色)的corresp值为1.175。这使得平均输入大约为0，而方差大约为1，从而加速了学习，要求<strong>手写体应该在中心</strong>，即20x20以内。</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-04-25</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/>图像分类</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/201409-vgg/ class=prev rel=prev title="Very Deep Convolutional Networks for Large-Scale Image Recognition"><i class="fas fa-angle-left fa-fw"></i>Very Deep Convolutional Networks for Large-Scale Image Recognition</a>
<a href=/201409-googlenet/ class=next rel=next title="Going deeper with convolutions">Going deeper with convolutions<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/about/ target=_blank rel="noopener noreferrer">Yangliuly1</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},data:{"desktop-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!","mobile-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},table:{sort:!0},typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},duration:-1,speed:100}}</script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/typeit/typeit.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript></div></body></html>