# Gradient-Based Learning Applied to Document Recognition

摘要：LeNet-5 是一个专为手写数字识别而设计的最经典的卷积神经网络，被誉为早期卷积神经网络中最有代表性的实验系统之一。
<!--more-->

## 文献信息

| 信息 | 内容                                                         |
| ---- | ------------------------------------------------------------ |
| 日期 | 19998.11                                                     |
| 作者 | Yann LeCun et al.  杨立昆，2018年图灵奖获得者，Facebook首席人工智能科学家 |
| 机构 | 贝尔实验室                                                   |
| 来源 | Proceedings of the IEEE                                      |
| 链接 | [Gradient-Based Learning Applied to Document Recognition](https://ieeexplore.ieee.org/document/726791) |
| 代码 | [Code]()                                                     |

## 个人理解

><strong style="color:red;">问题:</strong> 文章解决了全连接网络的尺寸大、不具备平移不变性，忽略图像的拓扑结构等问题；
> 
><strong style="color:red;">方法:</strong> 文章重点提出了卷积神经网络；
> 
><strong style="color:red;">结论:</strong> 在MNIST数据集上， LeNet-5 模型可以达到大约99.4%的准确率；
> 
><strong style="color:red;">理解:</strong> 卷积神经网络的鼻祖，为后续深度学习打下了基础，重点是局部连接的思想贯穿全文，不仅仅卷积神经网络，还有不完全连接等操作。
---

## 背景知识

传统的机器学习方式，手工设计特征+分类器。全连接网络经过梯度下降训练的多层网络学习复杂高维非线性映射的能力，使它们明显适合于传统模式识别模型中的图像识别任务。

**全连接网络缺点**

1. 图像往往尺寸很大，如果把图像看成行向量作为输入层，即使第一层全连接选择尽量少的神经元，第一层的参数也很多，导致整体参数都很多，计算量很大，模型也只适用于小图像。

2. 对于图像来说，不具有平移和局部失真的不变性（全连接网络每个神经元感受到的都是整幅图像，对平移，形变不具有不变性。只要对同一幅图像加入一些扰动，输出就会不同）。

3. 完全忽略了输入的拓扑结构（将一幅图像转换为行向量，行向量特征的顺序其实是可以打乱的，但是应该所有图像转换为行向量打乱的方式要相同。在不改变神经元的输出的前提下，输入数据可以是任意的顺序），这样训练网络其实对结果没有影响。但是，对于图像来说，一个像素与其相邻的像素往往是具有空间上的相关性的，那么可以从这个空间相关性出发提取到图像的一些局部的特征信息（如角特征和边缘特征等，应该传统方法中有局部特征提取的算子）。显然提取图像局部特征是全连接网络做不到的。

## 原理方法

LeNet-5 是一个专为手写数字识别而设计的最经典的卷积神经网络，被誉为早期卷积神经网络中最有代表性的实验系统之一。LeNet-5 模型由Yann LeCun 教授于 1998 年在其论文《Gradient-Based Learning Appliedto Document Recognition》中提出，这篇论文对于现代卷积神经网络的研究仍具有指导意义，可以说是CNN领域的第一篇经典之作。在MNIST数据集上， LeNet-5 模型可以达到大约99.4%的准确率，基于此神经网络模型而设计出的手写数字识别系统在 20 世纪 90年代被广泛应用于美国的多家银行进行支票手写字识别。根据 Yann LeCun 教授公开发表的论文的内容，可知 LeNet-5 模型共有8层（包括输入层和输出层），下图展示了LeNet-5模型的整体框架结构。

<div align=center>
    <img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com//images/20220706213811.png width=75% />
</div>
### 1、卷积网络

为了保证位移、尺度、畸变(shift, scale, distortion   invariance)不变性，提出了卷积神经网络，其三大核心思想：

1. 稀疏连接或局部感受野：基于图像局部相关的原理，保留了图像局部结构，同时减少了网络的权值。

2. 权值共享：基于图像局部相关的原理，同时减少网络的权值参数。

3. 下采样：对平移和形变更加鲁棒，实现特征的不变性，同时起到了一定的降维的作用。。

CNN相对于传统机器学习的不同：传统的机器学习需要手工设计特征；CNN是把卷积核作为特征提取算子来对图像提取特征，由于卷积核的参数是学习得到的，因此相当于卷积神经网络是自己训练得到了特征提取器，使用的是网络自己学习到的特征。

### 2、网络结构

LeNet5由7层CNN（不包含输入层）组成，输入原始图像大小是32×32像素，卷积层用Ci表示，下采样层（pooling，池化）用Si表示，全连接层用Fi表示。

```
C1层（卷积层5x5）：6@28×28 无激活函数
S2层（下采样层，池化层）：6@14×14 sigmod函数
C3层（卷积层5x5）：16@10×10 无 （不完全连接）
S4（下采样层，池化层）：16@5×5 sigmod函数
C5层（卷积层，实际为全连接层）：120 双曲正切函数 
F6层（全连接层）：84 双曲正切函数 
OUTPUT层（输出层）：10 径向基函数（RBF）
```

**不完全连接**：

目的：（1）使得网络的连接数保持在合理的范围内，应该就是减小运算量。（2）更重要的是，打破网络对称性，希望不同的feature map能够因为与前一层不同的feature map相连而学习到不同的特征。

原因：将全连接换成具有局部感受野的卷积，可以提高网络的性能。卷积的特点是相比全连接来说，卷积在Height和Width的维度上，神经元的连接是稀疏的（下一层神经元只与上一层的感受野内的神经元相连，而不是和所有的神经元相连）。LeNet在channel维度上也引入这种稀疏连接的形式呢，下一层的神经元在channel维度上，不是与上一层的所有channel的神经元相连的，而是只与部分channel的神经元相连。

问题：将全连接换卷积的优势是，卷积在进行运算时，感受野内的像素由于在空间上是相邻的，其灰度具有高度的相关性，因此更有利于提取特征。但是在channel的维度上，这可就不一定了。相邻的channel之间应该是没什么关联的，所以如果在channel层面上也采用稀疏连接，到底应该选那几个channel呢？文章的作者应该也是不太清楚这一点，所以在设计网络的时候是先选相邻的三层，然后相邻的四层，然后不完全相邻的四层，最后是五层全连。

但是目前来看，LeNet的这种设计是被现代CNN抛弃了，原因可能是有两点：（1）很可能就是GoogLeNet中提到的，这种设计不利于硬件设备的运算，反而效率更低；（2）不知道哪几个channel的特征是相关性更强的，还需要计算不同层特征的相关性（可能就是风格转换中的Gram矩阵），很麻烦。

**径向基神经网络：**基于距离进行衡量两个数据的相近程度的，RBF网最显著的特点是隐节点采用输人模式与中心向量的距离（如欧氏距离）作为函数的自变量，并使用径向基函数（如函数）作为激活函数。径向基函数关于N维空间的一个中心点具有径向对称性，而且神经元的输人离该中心点越远，神经元的激活程度就越低。

### 3、损失函数

基于最大似然估计准则，在LeNet论文的情形中应该是最小均方误差(MSE)：

$$
E(W)=\frac{1}{P} \sum_{p=1}^{P} y_{D^{p}}\left(Z^{p}, W\right)
$$
其中，$y_{D^{p}}$表示第$D_{i}-th$的RBF输出。
论文中提到了这个loss缺失了三个很重要的属性，但是作者通过在后面加了一项，对loss做了一个改进：
$$
E(W)=\frac{1}{P} \sum_{p=1}^{P}\left(y_{D^{p}}\left(Z^{p}, W\right)+\log \left(e^{-j}+\sum_{i} e^{-y_{i}\left(Z^{p}, W\right)}\right)\right)
$$
第二项的负数起到竞争的作用，它必然小于或等于第一项，因此这个损失函数是正的，常数j是正的，并且防止了准备好的非常大的类的惩罚被进一步推高。这种判别准则防止了在学习RBF参数时出现上述折叠现象，因为它使RBF中心彼此分离。

## 训练测试

在论文里说输入像素的值背景层(白色)的corresp值为-0.1，前景层(黑色)的corresp值为1.175。这使得平均输入大约为0，而方差大约为1，从而加速了学习，要求**手写体应该在中心**，即20x20以内。

## 参考文献

[^01]: [作者-文章-来源](地址)


