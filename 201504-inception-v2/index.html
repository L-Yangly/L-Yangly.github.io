<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>Batch Normalization, Accelerating Deep Network Training by Reducing Internal Covariate Shift - Yang</title><meta name=Description content><meta property="og:title" content="Batch Normalization, Accelerating Deep Network Training by Reducing Internal Covariate Shift"><meta property="og:description" content="摘要：BN归一化，加速收敛，防止梯度消失。"><meta property="og:type" content="article"><meta property="og:url" content="https://Yangliuly1.github.io/201504-inception-v2/"><meta property="og:image" content="https://Yangliuly1.github.io/images/avatar.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-27T10:18:50+08:00"><meta property="article:modified_time" content="2022-04-27T10:18:50+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Yangliuly1.github.io/images/avatar.png"><meta name=twitter:title content="Batch Normalization, Accelerating Deep Network Training by Reducing Internal Covariate Shift"><meta name=twitter:description content="摘要：BN归一化，加速收敛，防止梯度消失。"><meta name=application-name content="Yang"><meta name=apple-mobile-web-app-title content="Yang"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://Yangliuly1.github.io/201504-inception-v2/><link rel=prev href=https://Yangliuly1.github.io/201409-googlenet/><link rel=next href=https://Yangliuly1.github.io/201512-inception-v3/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Batch Normalization, Accelerating Deep Network Training by Reducing Internal Covariate Shift","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/Yangliuly1.github.io\/201504-inception-v2\/"},"genre":"posts","keywords":"图像分类","wordcount":151,"url":"https:\/\/Yangliuly1.github.io\/201504-inception-v2\/","datePublished":"2022-04-27T10:18:50+08:00","dateModified":"2022-04-27T10:18:50+08:00","publisher":{"@type":"Organization","name":"Yangliuly1"},"author":{"@type":"Person","name":"Yangliuly1"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark")}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i> </a><span class="menu-item delimiter"></span><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i></a><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1批归一化-batch-normalization>1、批归一化 Batch Normalization</a></li><li><a href=#2inception-v2结构>2、Inception v2结构</a></li></ul></li><li><a href=#训练测试>训练测试</a></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Batch Normalization, Accelerating Deep Network Training by Reducing Internal Covariate Shift</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=/about/ title=Author rel=author class=author>Yangliuly1</a>
</span>&nbsp;<span class=post-category>included in </span>&nbsp;<span class=post-category>category <a href=/categories/%E4%B8%93%E4%B8%9A%E8%AE%BA%E6%96%87/><i class="far fa-folder fa-fw"></i>专业论文</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-04-27>2022-04-27</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2022-04-27>2022-04-27</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;151 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;One minute&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1批归一化-batch-normalization>1、批归一化 Batch Normalization</a></li><li><a href=#2inception-v2结构>2、Inception v2结构</a></li></ul></li><li><a href=#训练测试>训练测试</a></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></div><div class=content id=content><p>摘要：BN归一化，加速收敛，防止梯度消失。</p><h1 id=batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift01 class=headerLink><a href=#batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift01 class=header-mark></a>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></h1><h2 id=文献信息 class=headerLink><a href=#%e6%96%87%e7%8c%ae%e4%bf%a1%e6%81%af class=header-mark></a>文献信息</h2><table><thead><tr><th>信息</th><th>内容</th></tr></thead><tbody><tr><td>日期</td><td>2015.04</td></tr><tr><td>作者</td><td>Sergey Ioffe et al. (<a href=mailto:sioffe@google.com rel>sioffe@google.com</a>)</td></tr><tr><td>机构</td><td>Google Inc</td></tr><tr><td>来源</td><td>arXiv</td></tr><tr><td>链接</td><td><a href=https://arxiv.org/abs/1502.03167 target=_blank rel="noopener noreffer">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></td></tr><tr><td>代码</td><td><a href rel>Code</a></td></tr></tbody></table><h2 id=个人理解 class=headerLink><a href=#%e4%b8%aa%e4%ba%ba%e7%90%86%e8%a7%a3 class=header-mark></a>个人理解</h2><blockquote><p><strong style=color:red>问题:</strong> 神经网络训练困难，原因是初始化和内部协变量偏移，SGD参数不容易收敛，易饱和状态。</p><p><strong style=color:red>方法:</strong> 批归一化；</p><p><strong style=color:red>结论:</strong> ILSVRC 2012，4.9%top-5验证集错误率和4.8%测试集错误率；</p><p><strong style=color:red>理解:</strong> 批归一化的作用，加速收敛和防止梯度消失。</p><p><strong style=color:red>优化：</strong>按批次归一化，受限于每一批次的数据，当批次数据之间样本差异大，可能效果也会很差。</p></blockquote><hr><h2 id=背景知识 class=headerLink><a href=#%e8%83%8c%e6%99%af%e7%9f%a5%e8%af%86 class=header-mark></a>背景知识</h2><p>网络收敛方式：1.较低的学习率，但减慢了收敛速度；2.谨慎的参数初始化；3.ReLU代替sigmoid 。</p><p>但是在要求低学习率以及比较好的参数初始化情况下，训练神经网络也是非常困难的。</p><p>因为训练深度神经网络时，会出现内部协变量偏移(Internal Covariate Shift)，即 <strong>在训练过程中，每层的输入分布因为前层的参数变化而不断变化。</strong> 具体来说，对于一个神经网络，第n层的输入就是第n-1层的输出，在训练过程中，每训练一轮参数就会发生变化，对于一个网络相同的输入，但n-1层的输出却不一样，这就导致第n层的输入也不一样，这个问题就叫做“Internal Covariate Shift”。</p><ol><li>SGD训练多层网络：总损失是$l=F_{2}(F_{1}(u, \theta <em>{1}),\theta</em>{2})$，当$F_{1}(u, \theta <em>{1})=x$，损失转换为$l=F</em>{2}(x, \theta <em>{2})$,则梯度更新是$\theta</em>{2}=\theta_{2}-\frac{\alpha}{m}\sum_{i=1}^{m}\frac{\partial F_{2}(x_{i},\theta_{2})}{\partial \theta_{2}}$。 当x的分布固定时候，训练$\theta_{2}$是容易收敛的。而当x的分布不断变化时候，$\theta_{2}$需要不断调整去修正x分布的变化带来的影响.</li><li>易进入饱和状态：考虑一个激活层$z=g(Wu+b)$，u代表输入层，W是权值矩阵，b是偏置矩阵，g(x)以sigmoid为例，$g(x)=\frac{1}{1+exp(-x)}$，当|x|增加，$g&rsquo;(x)$趋向于0，称为饱和状态。（梯度消失，模型将缓慢训练）。x受权值W和偏置b以及之前所有层的参数的影响，训练期间前面层参数的变化可能会将x的许多维度移动到非线性的饱和状态并收敛减慢。且这个前层的影响随着网络深度的增加而放大。实际中，饱和问题和导致梯度减小通常利用ReLu、初始化以及小的学习率来解决。</li></ol><h2 id=原理方法 class=headerLink><a href=#%e5%8e%9f%e7%90%86%e6%96%b9%e6%b3%95 class=header-mark></a>原理方法</h2><h3 id=1批归一化-batch-normalization class=headerLink><a href=#1%e6%89%b9%e5%bd%92%e4%b8%80%e5%8c%96-batch-normalization class=header-mark></a>1、批归一化 Batch Normalization</h3><p>减少内部协变量偏移，通过固定输入x的统计量（均值和方差），众所周知，如果输入白化\归一化，那么网络训练的收敛速度更快。所以在每一层的输入都进行白化处理，从而消除内部协变量带来的偏移问题。</p><p><strong>白化操作&ndash;whitened（LeCun et al，1998b）</strong>，对输入进行白化即对输入数据变换成0均值、单位方差的正态分布，一是特征相关性较低；二是特征具有相同的方差。使用白化来进行标准化（normalization），以此来消除internal covariate shift的不良影响，白化操作可以加快收敛，对于深度网络，每个隐层的输出都是下一个隐层的输入，即每个隐层的输入都可以做白化操作。由于whitened需要计算协方差矩阵和它的平方根的逆，而且在每次参数更新后需要对整个训练集进行分析，代价昂贵。</p><p>Batch Normalization，在白化的基础上做简化：</p><ul><li><p>简化1，单独标准化每个标量特征（每个通道），简单标准化可能改变该层的表达能力，以sigmoid层为例会把输入约束到线性状态。因此我们需要添加新的恒等变换去抵消这个（scale操作）。</p></li><li><p>简化2，mini-batch方式进行normalization，为了数值稳定， 是一个加到方差上的常量。</p></li></ul><p>BN的主要作用就是：</p><ul><li>加速网络训练</li><li>防止梯度消失</li></ul><p>如果激活函数是sigmoid，对于每个神经元，可以把逐渐向非线性映射的两端饱和区靠拢的输入分布，强行拉回到0均值单位方差的标准正态分布，即激活函数的兴奋区，在sigmoid兴奋区梯度大，即加速网络训练，还防止了梯度消失。基于此，BN对于sigmoid函数作用大，sigmoid函数在区间[-1, 1]中，近似于线性函数。就会降低了模型的表达能力，使得网络近似于一个线性映射，因此加入了scale 和shift。它们的主要作用就是找到一个线性和非线性的平衡点，既能享受非线性较强的表达能力，有可以避免非线性饱和导致网络收敛变慢问题。</p><blockquote><p>BN优点：</p><ol><li>减少了人为选择参数。在某些情况下可以取消 dropout 和 L2 正则项参数,或者采取更小的 L2 正则项约束参数；</li><li>减少了对学习率的要求。现在我们可以使用初始很大的学习率或者选择了较小的学习率，算法也能够快速训练收敛；</li><li>破坏原来的数据分布，一定程度上缓解过拟合（防止每批训练中某一个样本经常被挑选到，文献说这个可以提高 1% 的精度）。</li><li>减少梯度消失，加快收敛速度，提高训练精度。</li></ol><p>BN训练、测试和推理时区别：</p><ul><li><p>在训练时：对每一批的训练数据进行归一化，也即用每一批数据的均值和方差，并用指数滑动平均来近似统计整个空间的均值和方差。BN一般要求将训练集完全打乱，并用一个较大的batch值，否则，一个batch的数据无法较好得代表训练集的分布，会影响模型训练的效果。</p></li><li><p>在测试时：比如进行一个样本的预测，就并没有batch的概念，按照BN的公式，当测试样本batch为1的时候，大大降低了泛化能力，而当batch大于1的时候，样本的输出随所处batch变化，存在差异。因此，这个时候用的是指数滑动平均法（EMA）累计得到的“全局”<strong>训练</strong>数据的均值和方差。</p></li><li><p>在推理时：bn看作1x1卷积，$\alpha$是卷积权值，$\beta$是偏差，把bn和前一个卷积融合为新的卷积，减少计算量并加速。</p></li></ul><p>BN参数量：1x1卷积的参数；
BN与Dropout冲突：</p><ol><li>BN和dropout搭配使用时，模型的性能不升反降，因此后续缺省Dropout。但Wide ResNet（WRN）的作者多了一个“心眼”，他发现在很宽的WRN网络里面，在每一个bottleneck的两个conv层之间加上那么一个Dropout，竟然能得到稳定的提升。</li><li>原因是方差偏移，第一，训练时采用dropout，虽然通过除以(1-p)的方式来使得训练和测试时，每个神经元输入的期望大致相同，但是他们的方差却不一样。第二，BN是采用训练时得到的均值和方差对数据进行归一化的，现在dropout层的方差都不一样，后续变化也不一样。</li><li>解决方案：针对方差偏移，<a href=https://arxiv.org/abs/1801.05134 target=_blank rel="noopener noreffer">Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift</a>给出了两种解决方案，（1）拒绝方差偏移，只在所有BN层的后面采用dropout层，现在大部分开源的模型，都在网络的中间加了BN，你也就只能在softmax的前一层加加dropout。还有另外一种方法是模型训练完后，固定参数，以测试模式对训练数据求BN的均值和方差，再对测试数据进行归一化，论文证明这种方法优于baseline；（2）dropout原文提出了一种高斯dropout，论文再进一步对高斯dropout进行扩展，提出了一个均匀分布Dropout，这样做带来了一个好处就是这个形式的Dropout（又称为“Uout”）对方差的偏移的敏感度降低了，总得来说就是整体方差偏地没有那么厉害了。可以看得出来实验性能整体上比第一个方案好，这个方法显得更加稳定。</li></ol></blockquote><h3 id=2inception-v2结构 class=headerLink><a href=#2inception-v2%e7%bb%93%e6%9e%84 class=header-mark></a>2、Inception v2结构</h3><p>V1到V2的改动：</p><ul><li>2个3X3代替5X5，网络的最大深度增加9个权重层。同时，参数数目增加了25%，计算量增加了30%左右。目的是类似于VGG，保持相同感受野的同时减少参数和加强非线性的表达能力。</li><li>28X28modules从2个增加到3个。</li><li>在modules中，pooling有时average ，有时maximum。</li><li>没有across board pooling layers在任意两个inception modules。只在3c，4e里会有stride-2的卷积和pooling。</li></ul><h2 id=训练测试 class=headerLink><a href=#%e8%ae%ad%e7%bb%83%e6%b5%8b%e8%af%95 class=header-mark></a>训练测试</h2><p>略。</p><h2 id=参考文献 class=headerLink><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae class=header-mark></a>参考文献</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://zhuanlan.zhihu.com/p/33101420 target=_blank rel="noopener noreffer">李翔-大白话《Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift》-知乎</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-04-27</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/>图像分类</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/201409-googlenet/ class=prev rel=prev title="Going deeper with convolutions"><i class="fas fa-angle-left fa-fw"></i>Going deeper with convolutions</a>
<a href=/201512-inception-v3/ class=next rel=next title="Rethinking the Inception Architecture for Computer">Rethinking the Inception Architecture for Computer<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/about/ target=_blank rel="noopener noreferrer">Yangliuly1</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},data:{"desktop-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!","mobile-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},table:{sort:!0},typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},duration:-1,speed:100}}</script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/typeit/typeit.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript></div></body></html>