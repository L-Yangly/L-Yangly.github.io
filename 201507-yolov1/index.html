<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>You Only Look Once - Unified, Real-Time Object Detection - Yang</title><meta name=Description content><meta property="og:title" content="You Only Look Once - Unified, Real-Time Object Detection"><meta property="og:description" content="摘要：Yolov1的思想是整张图作为网络的输入，直接在输出层回归边界框的位置和类别；"><meta property="og:type" content="article"><meta property="og:url" content="https://Yangliuly1.github.io/201507-yolov1/"><meta property="og:image" content="https://Yangliuly1.github.io/images/avatar.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-18T23:49:26+08:00"><meta property="article:modified_time" content="2022-08-18T23:49:26+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Yangliuly1.github.io/images/avatar.png"><meta name=twitter:title content="You Only Look Once - Unified, Real-Time Object Detection"><meta name=twitter:description content="摘要：Yolov1的思想是整张图作为网络的输入，直接在输出层回归边界框的位置和类别；"><meta name=application-name content="Yang"><meta name=apple-mobile-web-app-title content="Yang"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://Yangliuly1.github.io/201507-yolov1/><link rel=prev href=https://Yangliuly1.github.io/201905-efficientnet/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"You Only Look Once - Unified, Real-Time Object Detection","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/Yangliuly1.github.io\/201507-yolov1\/"},"genre":"posts","keywords":"目标检测","wordcount":270,"url":"https:\/\/Yangliuly1.github.io\/201507-yolov1\/","datePublished":"2022-08-18T23:49:26+08:00","dateModified":"2022-08-18T23:49:26+08:00","publisher":{"@type":"Organization","name":"Yangliuly1"},"author":{"@type":"Person","name":"Yangliuly1"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark")}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i> </a><span class="menu-item delimiter"></span><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i></a><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#0x01-文献信息>0x01 文献信息</a></li><li><a href=#0x02-个人理解>0x02 个人理解</a></li><li><a href=#0x03-背景知识>0x03 背景知识</a></li><li><a href=#0x04-原理方法>0x04 原理方法</a><ul><li><a href=#1-原理01>(1) 原理</a></li><li><a href=#2-网络结构>(2) 网络结构</a></li><li><a href=#3-损失函数>(3) 损失函数</a></li></ul></li><li><a href=#0x05-训练测试>0x05 训练测试</a></li><li><a href=#0xff-参考文献>0xFF 参考文献</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">You Only Look Once - Unified, Real-Time Object Detection</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=/about/ title=Author rel=author class=author>Yangliuly1</a>
</span>&nbsp;<span class=post-category>included in </span>&nbsp;<span class=post-category>category <a href=/categories/%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87/><i class="far fa-folder fa-fw"></i>学术论文</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-08-18>2022-08-18</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2022-08-18>2022-08-18</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;270 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;2 minutes&nbsp;</div></div><div class=featured-image><img class=lazyload data-src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/imgs/20220821102318.png data-srcset="https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/imgs/20220821102318.png, https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/imgs/20220821102318.png 1.5x, https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/imgs/20220821102318.png 2x" data-sizes=auto alt=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/imgs/20220821102318.png title=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/imgs/20220821102318.png height=auto width=auto></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#0x01-文献信息>0x01 文献信息</a></li><li><a href=#0x02-个人理解>0x02 个人理解</a></li><li><a href=#0x03-背景知识>0x03 背景知识</a></li><li><a href=#0x04-原理方法>0x04 原理方法</a><ul><li><a href=#1-原理01>(1) 原理</a></li><li><a href=#2-网络结构>(2) 网络结构</a></li><li><a href=#3-损失函数>(3) 损失函数</a></li></ul></li><li><a href=#0x05-训练测试>0x05 训练测试</a></li><li><a href=#0xff-参考文献>0xFF 参考文献</a></li></ul></nav></div></div><div class=content id=content><p>摘要：Yolov1的思想是整张图作为网络的输入，直接在输出层回归边界框的位置和类别；</p><h1 id=you-only-look-once-unified-real-time-object-detection class=headerLink><a href=#you-only-look-once-unified-real-time-object-detection class=header-mark></a>You Only Look Once: Unified, Real-Time Object Detection</h1><h2 id=0x01-文献信息 class=headerLink><a href=#0x01-%e6%96%87%e7%8c%ae%e4%bf%a1%e6%81%af class=header-mark></a>0x01 文献信息</h2><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/imgs/20220821102318.png width=75%></div><table><thead><tr><th>信息</th><th>内容</th></tr></thead><tbody><tr><td>日期</td><td>Jun 2015</td></tr><tr><td>作者</td><td>Joseph Redmon et al.</td></tr><tr><td>机构</td><td>xxxxxxxx</td></tr><tr><td>来源</td><td>arXiv</td></tr><tr><td>链接</td><td><a href=https://arxiv.org/abs/1506.02640 target=_blank rel="noopener noreffer">https://arxiv.org/abs/1506.02640</a></td></tr><tr><td>代码</td><td><a href rel>Code</a></td></tr></tbody></table><h2 id=0x02-个人理解 class=headerLink><a href=#0x02-%e4%b8%aa%e4%ba%ba%e7%90%86%e8%a7%a3 class=header-mark></a>0x02 个人理解</h2><blockquote><p><strong style=color:red>问题:</strong> 两阶段R-CNN系列网络的缺陷，生成候选框问题，再进行分类器对候选框划分，网络速度慢且难于优化因为需要分开训练；</p><p><strong style=color:red>方法:</strong> 文章提出了一阶段网络，回归坐标和类别思想；</p><p><strong style=color:red>结论:</strong> 速度达到45FPS和155FPS，精度提升两倍；</p><p><strong style=color:red>理解:</strong> 一阶段的开山鼻祖，回归坐标和类别，以网格划分图片，对每个网格来预测相应对象；</p><ul><li>YOLO对相互靠的很近的物体，还有很小的群体 检测效果不好，这是因为一个网格中只预测了两个框，并且只属于一类。</li><li>由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。</li><li>对测试图像中，同一类物体出现的新的不常见的长宽比和其他情况，泛化能力偏弱。</li></ul></blockquote><hr><h2 id=0x03-背景知识 class=headerLink><a href=#0x03-%e8%83%8c%e6%99%af%e7%9f%a5%e8%af%86 class=header-mark></a>0x03 背景知识</h2><p>目标检测：确定物体的类别和位置；</p><p>传统的思路：基于滑窗的思想，再对窗口分类，DPM思路；</p><p>两阶段网络：R-CNN系列，先生成候选框，再对候选框分类和微调坐标；</p><h2 id=0x04-原理方法 class=headerLink><a href=#0x04-%e5%8e%9f%e7%90%86%e6%96%b9%e6%b3%95 class=header-mark></a>0x04 原理方法</h2><h3 id=1-原理01 class=headerLink><a href=#1-%e5%8e%9f%e7%90%8601 class=header-mark></a>(1) 原理<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></h3><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/imgs/20220819000358.png width=75%></div><ol><li><p>将一幅图像分成SxS个网格(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object；</p></li><li><p>每个网格要预测B个<strong>边界框</strong>bounding box，每个bounding box除了要回归自身的位置之外，还要附带预测一个confidence值；
这个confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息，其值是这样计算的：$Pr(Object) * IOU^{truth}_{pred}$；</p><p>其中，如果有object落在一个grid cell里，第一项取1，否则取0。 第二项是预测的bounding box和实际的groundtruth之间的IoU值；</p></li><li><p>每个bounding box要预测(x, y, w, h)和confidence共5个值，每个网格还要预测一个类别信息，记为C类。则SxS个网格，每个网格要预测B个bounding box还要预测C个categories。输出就是S x S x (5*B+C)的一个tensor。
注意：class信息是针对每个网格的，confidence信息是针对每个bounding box的。</p></li></ol><p>实例：在PASCAL VOC中， 图像输入为448x448，取S=7，B=2，一共有20个类别(C=20)。则输出就是7x7x30的一个tensor。</p><h3 id=2-网络结构 class=headerLink><a href=#2-%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84 class=header-mark></a>(2) 网络结构</h3><p>网络结构借鉴了 GoogLeNet：24个卷积层，2个全链接层。（用1×1 reduction layers 紧跟 3×3 convolutional layers 取代Goolenet的 inception modules ）</p><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/imgs/20220820221324.png width=75%></div>网络输出:每个grid有30维，这30维中，8维是回归box的坐标，2维是box的confidence，还有20维是类别。
其中坐标的x,y用对应网格的offset归一化到0-1之间，w,h用图像的width和height归一化到0-1之间；<h3 id=3-损失函数 class=headerLink><a href=#3-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 class=header-mark></a>(3) 损失函数</h3><p>平方和误差损失: 坐标预测 + 置信度预测 + 类别预测；
$$
\begin{aligned}</p><p>loss & = \lambda_{coord} \sum^{S^2}<em>{i=0}\sum^B</em>{j=0}\prod^{obj}<em>{ij}{{(x_i - \hat{x}<em>i)^2 + (y_i - \hat{y}<em>i)^2}} \
& \qquad + \lambda</em>{coord} \sum^{S^2}</em>{i=0}\sum^B</em>{j=0}\prod^{obj}_{ij}{{(\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{H_i} - \sqrt{\hat{H}_i})^2}} \</p><p>& \qquad + \sum^{S^2}<em>{i=0}\sum^B</em>{j=0}\prod^{obj}_{ij} (C_i - \hat{C_i})^2</p><ul><li>\lambda_{noobj} \sum^{S^2}<em>{i=0}\sum^B</em>{j=0}\prod^{noobj}_{ij} (C_i - \hat{C_i})^2 \</li></ul><p>& \qquad + \sum^{S^2}<em>{i=0} \prod^{obj}</em>{ij} \sum_{c \in classes} (p_i(c) - \hat{p_i}(c))^2</p><p>\end{aligned}
$$</p><ul><li>只有当某个网格中有object的时候才对classification error进行惩罚;</li><li>只有当某个box predictor对某个ground truth box负责的时候，才会对box的coordinate error进行惩罚，而对哪个ground truth box负责就看其预测值和ground truth box的IoU是不是在那个cell的所有box中最大;</li></ul><p>损失函数的设计目标就是让坐标（x,y,w,h），confidence，classification 这个三个方面达到很好的平衡。简单的全部采用了sum-squared error loss来做这件事会有以下不足：</p><p>a) 8维的localization error和20维的classification error同等重要显然是不合理的； b) 如果一个网格中没有object（一幅图中这种网格很多），那么就会将这些网格中的box的confidence push到0，相比于较少的有object的网格，这种做法是overpowering的，这会导致网络不稳定甚至发散。</p><p>解决方案如下：</p><ul><li>更重视8维的坐标预测，给这些损失前面赋予更大的loss weight, 记为$\lambda_{coord}$，在pascal VOC训练中取5。</li><li>对没有object的bbox的confidence loss，赋予小的loss weight，记为$\lambda_{noobj}$，在pascal VOC训练中取0.5。</li><li>有object的bbox的confidence loss和类别的loss的loss weight正常取1。</li><li>对不同大小的bbox预测中，相比于大bbox预测偏一点，小box预测偏一点更不能忍受。而sum-square error loss中对同样的偏移loss是一样。 为了缓和这个问题，作者用了一个比较取巧的办法，就是将box的width和height取平方根代替原本的height和width。因为small bbox的横轴值较小，发生偏移时，反应到y轴上的loss比big box要大。</li></ul><p>一个网格预测多个bounding box，在训练时我们希望每个object（ground true box）只有一个bounding box专门负责（一个object 一个bbox）。具体做法是与ground true box（object）的IOU最大的bounding box负责该ground true box(object)的预测。这种做法称作bounding box predictor的specialization(专职化)。每个预测器会对特定（sizes, aspect ratio or classed of object）的ground true box预测的越来越好。（个人理解：IOU最大者偏移会更少一些，可以更快速的学习到正确位置）</p><p>REF: <a href=https://github.com/abeardear/pytorch-YOLO-v1/blob/master/yoloLoss.py target=_blank rel="noopener noreffer">https://github.com/abeardear/pytorch-YOLO-v1/blob/master/yoloLoss.py</a></p><h2 id=0x05-训练测试 class=headerLink><a href=#0x05-%e8%ae%ad%e7%bb%83%e6%b5%8b%e8%af%95 class=header-mark></a>0x05 训练测试</h2><p>预训练分类网络：在 ImageNet 1000-class competition dataset上预训练一个分类网络，这个网络前20个卷机网络+average-pooling layer+ fully connected layer （此时网络输入是224x224）。</p><p>训练检测网络：转换模型去执行检测任务，《Object detection networks on convolutional feature maps》提到说在预训练网络中增加卷积和全链接层可以改善性能。在他们例子基础上添加4个卷积层和2个全链接层，随机初始化权重。检测要求细粒度的视觉信息，所以把网络输入也又224x224变成448x448。</p><p>测试过程：</p><ol><li>Resize成448*448，图片分割得到7x7网格(cell)</li><li>CNN提取特征和预测：卷积部分负责提特征。全链接部分负责预测：a) 7x7x2=98个bounding box(bbox) 的坐标$x_{center},y_{center},w,h$ 和是否有物体的conﬁdence 。 b) 7x7=49个cell所属20个物体的概率。</li><li>过滤bbox（通过nms），得到目标；</li></ol><p>在test的时候，先根据置信度选择含目标的网格区域（即每个cell只选最大概率的那个预测框，且阈值大于0.1），对应最大的分类概率作为该网格区域的类别，分别计算对应的坐标和得分，即每个网格预测的class信息和bounding box预测的confidence信息相乘，就得到每个bounding box的class-specific confidence score:
$$
Pr(Class_i | Object) * Pr(Object) * IOU^{true}<em>{pred} = Pr</em>{class_i} * IOU^{true}_{pred}
$$</p><p>其中，第一项就是每个网格预测的类别信息，第二、三项就是每个bounding box预测的confidence。这个乘积即encode了预测的box属于某一类的概率，也有该box准确度的信息。</p><p>根据每个box的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果。</p><h2 id=0xff-参考文献 class=headerLink><a href=#0xff-%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae class=header-mark></a>0xFF 参考文献</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://blog.csdn.net/frighting_ing/article/details/123450918 target=_blank rel="noopener noreffer">Fighting_1997-yolov1详解-CSDN</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-08-18</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/>目标检测</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/201905-efficientnet/ class=prev rel=prev title="EfficientNet, Rethinking Model Scaling for Convolutional Neural Networks"><i class="fas fa-angle-left fa-fw"></i>EfficientNet, Rethinking Model Scaling for Convolutional Neural Networks</a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/about/ target=_blank rel="noopener noreferrer">Yangliuly1</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},data:{"desktop-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!","mobile-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},table:{sort:!0},typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},duration:-1,speed:100}}</script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/typeit/typeit.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript></div></body></html>