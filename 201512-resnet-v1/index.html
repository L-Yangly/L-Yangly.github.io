<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>Deep Residual Learning for Image Recognition - Yang</title><meta name=Description content><meta property="og:title" content="Deep Residual Learning for Image Recognition"><meta property="og:description" content="摘要：残差学习的目的是让模型的内部结构至少有恒等映射的能力，以保证在堆叠网络的过程中，网络至少不会因为继续堆叠而产生退化!"><meta property="og:type" content="article"><meta property="og:url" content="https://Yangliuly1.github.io/201512-resnet-v1/"><meta property="og:image" content="https://Yangliuly1.github.io/images/avatar.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-23T11:59:33+08:00"><meta property="article:modified_time" content="2022-04-23T11:59:33+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Yangliuly1.github.io/images/avatar.png"><meta name=twitter:title content="Deep Residual Learning for Image Recognition"><meta name=twitter:description content="摘要：残差学习的目的是让模型的内部结构至少有恒等映射的能力，以保证在堆叠网络的过程中，网络至少不会因为继续堆叠而产生退化!"><meta name=application-name content="Yang"><meta name=apple-mobile-web-app-title content="Yang"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://Yangliuly1.github.io/201512-resnet-v1/><link rel=prev href=https://Yangliuly1.github.io/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86-0x-05%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/><link rel=next href=https://Yangliuly1.github.io/201603-resnet-v2/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Deep Residual Learning for Image Recognition","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/Yangliuly1.github.io\/201512-resnet-v1\/"},"genre":"posts","keywords":"图像分类","wordcount":129,"url":"https:\/\/Yangliuly1.github.io\/201512-resnet-v1\/","datePublished":"2022-04-23T11:59:33+08:00","dateModified":"2022-04-23T11:59:33+08:00","publisher":{"@type":"Organization","name":"Yangliuly1"},"author":{"@type":"Person","name":"Yangliuly1"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark")}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(t){const e=document.getElementsByTagName("meta");for(let n=0;n<e.length;n++)if(e[n].getAttribute("name")===t)return e[n];return''}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?setTheme("dark"):setTheme("light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?setTheme("dark"):setTheme("light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class="far fa-edit fa-fw"></i></span><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i> </a><span class="menu-item delimiter"></span><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class="far fa-edit fa-fw"></i></span><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i></a><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1思想>1、思想</a></li><li><a href=#2结构>2、结构</a></li></ul></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Deep Residual Learning for Image Recognition</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=/about/ title=Author rel=author class=author>Yangliuly1</a>
</span>&nbsp;<span class=post-category>included in </span>&nbsp;<span class=post-category>category <a href=/categories/%E4%B8%93%E4%B8%9A%E8%AE%BA%E6%96%87/><i class="far fa-folder fa-fw"></i>专业论文</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-04-23>2022-04-23</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2022-04-23>2022-04-23</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;129 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;One minute&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1思想>1、思想</a></li><li><a href=#2结构>2、结构</a></li></ul></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></div><div class=content id=content><p>摘要：残差学习的目的是让模型的内部结构至少有恒等映射的能力，以保证在堆叠网络的过程中，网络至少不会因为继续堆叠而产生退化!</p><h1 id=deep-residual-learning-for-image-recognitioncode class=headerLink><a href=#deep-residual-learning-for-image-recognitioncode class=header-mark></a>Deep Residual Learning for Image RecognitionCODE</h1><h2 id=文献信息 class=headerLink><a href=#%e6%96%87%e7%8c%ae%e4%bf%a1%e6%81%af class=header-mark></a>文献信息</h2><table><thead><tr><th>信息</th><th>内容</th></tr></thead><tbody><tr><td>日期</td><td>2015.12</td></tr><tr><td>作者</td><td><a href=http://kaiminghe.com/ target=_blank rel="noopener noreffer">Kaiming He</a> et al.</td></tr><tr><td>机构</td><td>Microsoft Research</td></tr><tr><td>来源</td><td>CVPR2016 最佳论文 (Computer Vision and Pattern Recognition)</td></tr><tr><td>链接</td><td><a href=https://arxiv.org/abs/1512.03385 target=_blank rel="noopener noreffer">Deep Residual Learning for Image Recognition</a></td></tr><tr><td>代码</td><td><a href rel>Code</a></td></tr></tbody></table><h2 id=个人理解 class=headerLink><a href=#%e4%b8%aa%e4%ba%ba%e7%90%86%e8%a7%a3 class=header-mark></a>个人理解</h2><blockquote><p><strong style=color:red>问题:</strong> 网络深度问题</p><ol><li>梯度爆炸和消散问题：由于随着层数的增多，在网络中反向传播的梯度会随着连乘变得不稳定，变得特别大或者特别小，从而梯度爆炸或消散。为了克服梯度问题也诞生了许多的解决办法，如归一化BatchNorm，激活函数ReLu，使用Xaiver初始化，随机梯度下降（SGD）等。</li><li>网络的退化问题：即网络深度增加时，网络准确度出现饱和，甚至出现下降。注意：并不是过拟合问题，因为深层网络的训练集和验证集的误差都比较高，而过拟合是训练集误差低验证集误差高。</li></ol><p><strong style=color:red>方法:</strong> 残差学习解决退化问题；</p><p><strong style=color:red>结论:</strong> ResNet在ILSVRC和COCO 2015取得5项第一；</p><p><strong style=color:red>理解:</strong></p><ol><li>网络退化问题：理论上来说，堆叠网络，哪怕这些增加层什么也不学习，仅仅复制浅层网络的特征，即恒等映射（Identity mapping），在这种极端情况下，深层网络应该和浅层网络性能一样，也不应该退化，可能训练方法有问题，才使得深层网络很难去找到一个好的参数。</li><li>残差学习，从特征的学习$H(x)$映射到残差的学习$F(x)$，对应的特征$H&rsquo;(x) = F(x) + x$，残差学习更容易学习：（1）当残差为0时，堆叠层相当于恒等映射，网络不会退化；（2）实际上残差并不为0，会使得网络在输入特征上学到新特征，达到更好的性能。</li></ol><p><strong style=color:red>优化：</strong></p><ol><li>为什么残差学习相对容易小？从梯度入手来$y&rsquo; = 1 + F&rsquo;(x)$。</li><li>不同理解：（1）即使BN过后梯度的模稳定在了正常范围内，但<strong>梯度的相关性实际上是随着层数增加持续衰减的</strong>。而经过证明，ResNet可以有效减少这种相关性的衰减；（2）跳连接相加可以实现不同分辨率特征的组合；（DenseNet，跳接组合concat更多分辨率的特征，模型的效果会更好？）（3）引入跳接实际上让模型自身<strong>有了更加“灵活”的结构</strong>，即在训练过程本身，模型可以选择在每一个部分是“更多进行卷积与非线性变换”还是“更多倾向于什么都不做”，抑或是将两者结合。<a href=https://www.zhihu.com/question/64494691/answer/786270699 target=_blank rel="noopener noreffer">link</a>。</li><li>问题1：既然非常深的模型有这样那样的劣势，那么为什么不直接减少层数？给我的感觉ResNet更多的是为了保证更深的模型不会变得更糟。或者说，减到多少层是一个比添加Residual block更糟心更费时的过程？（模型越深越能拟合复杂的表达，浅层网络达不到要求）</li><li>问题2：加了shortcut以后，back propagation的时候如何更新参数？求导结果上加了一个常数。</li><li>问题3：已有的神经网络很难拟合潜在的恒等映射函数H(x) = x？有激活函数，会卡掉一部分输入的值。</li><li>问题4：恒等映射了，那就是后面的层理想状态下输入输出没有改变，那还要这些层干嘛呢？恒等是加在那些层的输出上的，输出是卷积和恒等映射的共同结果？</li><li>问题5：有没有可能训练结束后发现有一个Residual Block作用相当于恒等映射，而之后的层并不是，这样的话是不是就能把这个block去掉了？</li><li>问题6：普通残差block至少包含两层卷积，一层卷积呢？“先求和再激活”还是“先激活再求和”，前者无意义，后者有意义。如果一层卷积，先求和再激活就没有意义，后者就是有意义的。</li></ol></blockquote><hr><h2 id=背景知识 class=headerLink><a href=#%e8%83%8c%e6%99%af%e7%9f%a5%e8%af%86 class=header-mark></a>背景知识</h2><p>深度卷积神经网络：特征“level”可以通过堆叠层数（深度）来丰富低、中和高级特征。</p><p>深度随之带来的问题1：梯度消失和爆炸，阻碍收敛。权重初始化、归一化和随机梯度下降都能有效解决该问题。</p><p>深度随之带来的问题1：网络退化，随着网络深度的增加，精度会达到饱和（这可能并不奇怪），然后迅速退化。这种退化并不是由过度拟合引起的，向适当深度的模型中添加更多层会导致更高的训练误差，训练精度的降低表明并非所有系统都同样容易优化。</p><p>Residual Representations：</p><ul><li>VLAD通过残差向量相对于字典进行编码；</li><li>Fisher向量可以表示为VLAD的概率版本；</li></ul><p>Shortcut Connections：</p><ul><li>中间层直接连接到辅助分类器，用于处理消失/爆炸梯度；</li><li>通过快捷连接实现的层响应、梯度和传播误差的中心化方法；</li></ul><h2 id=原理方法 class=headerLink><a href=#%e5%8e%9f%e7%90%86%e6%96%b9%e6%b3%95 class=header-mark></a>原理方法</h2><h3 id=1思想 class=headerLink><a href=#1%e6%80%9d%e6%83%b3 class=header-mark></a>1、思想</h3><p>借鉴VLAD（残差思想）和<a href=https://arxiv.org/pdf/1505.00387.pdf target=_blank rel="noopener noreffer">Highway Network</a>（跨层连接思想，相加$y=F(x)+Wx$），提出了shortcut connection结构。</p><p>$$
y = F(x) + x \
F(x) = Wx + b
$$
<strong>恒等映射（identity mappings）</strong>：如果F(x) = 0, y=x, 即为恒等映射（H(x) = x）。事实上，当层数太深了，F(x)趋近与0，使得无论x为何值时，F(x)总为0，y = x，不会出现退化现象。</p><p><strong>为什么残差容易学习？</strong>：值得一提的是，假如优化目标函数是逼近一个恒等映射, 而不是0映射， 那么学习找到对恒等映射的扰动会比重新学习一个映射函数要容易。更通俗来说对于某层特征$y_l=F_l(F_{l-1}(x_{l-1}) + Wx_{l-1})+Wx_l$，当网络梯度$y&rsquo;=1+F&rsquo;(x)$，即残差梯度不会恰巧都为-1，而且就算很小，有1的存在也不会导致梯度消失。</p><p>ResNet改变目标函数，目标值y和x的差值，即所谓的残差$F(x) := y-x$。因此，后面的训练目标就是要将残差结果逼近于0，使到随着网络加深，准确率不下降，使输入x近似于输出y，以保持在后面的层次中不会造成精度下降。</p><h3 id=2结构 class=headerLink><a href=#2%e7%bb%93%e6%9e%84 class=header-mark></a>2、结构</h3><p>残差单元：普通残差块（浅层）和瓶颈残差块（深层），</p><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/image-20220424000959498.png width=75%></div><p>短路链接：对于输入输出一致，直接相加；对于输入输出不一致的处理细节</p><ul><li>空间上不一样，跳连部分给x添加一个线性映射$y = F(x) + Wx$，即stride=2是来解决空间</li><li>维度上不一样，（1）补零填充；（2）1x1卷积升维。两种都可行，主要是会不会增加参数，从而影响计算量。</li></ul><p>网络结构：</p><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/v2-1dfd4022d4be28392ff44c49d6b4ed94_r.jpg width=75%></div><h2 id=参考文献 class=headerLink><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae class=header-mark></a>参考文献</h2><ul><li>[1]-<a href="https://blog.csdn.net/qq_41760767/article/details/97917419?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param" target=_blank rel="noopener noreffer">会哭泣的猫-ResNet网络详细解析-CSDN</a></li><li>[2]-<a href=https://zhuanlan.zhihu.com/p/31852747 target=_blank rel="noopener noreffer">小小将-你必须要知道CNN模型：ResNet-知乎</a></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-04-23</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/>图像分类</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86-0x-05%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ class=prev rel=prev title="图像处理 0x 05：图像分割"><i class="fas fa-angle-left fa-fw"></i>图像处理 0x 05：图像分割</a>
<a href=/201603-resnet-v2/ class=next rel=next title="Identity Mappings in Deep Residual Networks">Identity Mappings in Deep Residual Networks<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/about/ target=_blank rel="noopener noreferrer">Yangliuly1</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},data:{"desktop-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!","mobile-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},table:{sort:!0},typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},duration:-1,speed:100}}</script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/typeit/typeit.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript></div></body></html>