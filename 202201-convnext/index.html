<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>- Yang</title><meta name=Description content><meta property="og:title" content><meta property="og:description" content="先导知识 Transformer 残差网络 Swin Transformer ResNeXt MobileNet v2 Layer Normalization 前言FAIR于近日提出的ConvNeXt[1]得到了广泛的关注，它引起关注的原因是它靠卷积结构便达到了ImageNet Top-1的准确率，与这两年来流行的使用Transformer[2]解决视觉问题的趋势背道而驰，让不少读者感叹“爷青回！”。再加上有何恺明，RBG，Yann LeCun的站台，很难不让人关注这篇文章。
ConvNeXt并没有特别复杂或者创新的结构，它的每一个网络细节都是已经在不止一个网络中被采用。而就是靠这些边角料的互相配合，却也达到了ImageNet Top-1的准确率。它涉及这些边角料的动机也非常简单：Transformer或者Swin-Transformer [3]怎么做，我也对应的调整，效果好就保留。当然这些边角料的摸索也是需要大量的实验数据支撑的，是一个耗时耗力耗资源的过程。通过对ConvNeXt的学习，我等调参侠不仅可以学习到诸多的炼丹经验，还可以一探其背后原理，在CNN和Transformer的大战中以内行的角度吃瓜。
论文：https://arxiv.org/abs/2201.03545
源码：https://github.com/facebookresearch/ConvNeXt
图1概括了ConvNeXt的所有优化点，它从ResNet-50[4]或者ResNet-200出发，依次从宏观设计，深度可分离卷积（ResNeXt[5]），逆瓶颈层（MobileNet v2[6]），大卷积核，细节设计这五个角度依次借鉴Swin Transformer的思想，然后在ImageNet-1K上进行训练和评估，最终得到ConvNeXt的核心结构。
图1：ConvNeXt的网络结构优化策略
1.1 训练方式随着深度学习在各个领域上的不断探索，残差网络锁采用的原始策略已经不能充分的压榨残差结构的性能，例如前不久的ResNet-timm[10] 就通过一系列训练trick将ResNet在ImageNet-1k的Top-1的准确率提升到80%+。在介绍ConvNeXt的结构优化之前，我们先简单介绍一下ConvNeXt的训练方法。我们知道ResNet-50在ImageNet-1k上最终的Top-1的准确率是76.1%，而图1中ResNet-50说的是78.8%，而这2.7%的准确率的提升正是依靠对训练策略的优化所达到的。
在ConvNeXt中，它的优化策略借鉴了Swin-Transformer。具体的优化策略包括：（1）将训练Epoch数从90增加到300；（2）优化器从SGD改为AdamW；（3）更复杂的数据扩充策略，包括Mixup，CutMix，RandAugment，Random Erasing等；（4）增加正则策略，例如随机深度[7]，标签平滑[8]，EMA[9]等。更具体的预训练和微调的超参数如图2。
图2：ConvNeXt的训练参数
1.2 宏观设计 1.2.1 每个阶段的计算占比（Stage Ratio）VGG提出了把骨干网络分成若干个网络块的结构，每个网络块通过池化操作将Feature Map降采样到不同的尺寸。在VGG中，每个网络块的网络层的数量基本是相同，但在之后的很多工作中，他们指出当深层的网络块层数更多时，模型的表现更好。例如，ResNet-50中共有4个不同的网络块，每个网络块又有若干个不同的基础层，一般是由卷积，BN等操作组成，它的每个网络块的层数是 。
在Swin-Transformer中，每个骨干网络被分成了4个不同的Stage，每个Stage又是由若干个Block组成，在Swin-Transformer中，这个Block的比例是 ，而对于更大的模型来说，这个比例是 。ConvNeXt的改进是将ResNet-50的每个Stage的block的比例调整到 ，最终得到的block数是 。从图1中可以看出这个改进将ResNet-50的准确率从78.8%提升至79.4%。注意这里模型的GFLOPs从4.1增加至4.5，这0.6%的准确率的提升也有一部分功劳要归功于模型参数量的增加。此外ConvNeXt还提供了更大的block数为 的模型。
1.2.2 Patchify Stem对于ImageNet数据集，我们通常采用 的输入尺寸，这个尺寸对于ViT等基于Transformer的模型来说是非常大的，它们通常使用一个步长为4，大小也为4的卷积将其尺寸降采样到 。因为这个卷积的步长和大小是完全相同的，所以它又是一个无覆盖的卷积，或者叫Patchify（补丁化)的卷积。这一部分在Swin-Transformer中叫做stem层，它是位于输入之后的一个降采样层。
在ConvNeXt中，Stem层也是一个步长为4，大小也为4的卷积操作，这一操作将准确率从79.4%提升至79.5%，GFLOPs从4.5降到4.4%。也有人指出使用覆盖的卷积（例如步长为4，卷积核大小为7的卷积）能够获得更好的表现。
1 2 3 4 stem = nn.Sequential( nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4), LayerNorm(dims[0], eps=1e-6, data_format=&#34;channels_first&#34;) ) 1.2.3 分组卷积在轻量级模型中，我们介绍了深度可分离卷积，它们将 卷积以通道为单位进行运算（深度卷积），然后再通过 卷积进行通道融合（点卷积）。而ResNeXt是一个更折中的方案，它通过分组卷积（将通道分组，然后以组为单位进行卷积）的方式来提升模型的计算速度。与之类似的是，Swin-Tranformer的Self-Attention也是以通道为单位的运算单元，不同的是可分离卷积是可学习的卷积核，Self-Attention是根据数据动态计算的权值。
在ConvNeXt中，也引入了分组卷积的思想。它将 卷积替换成了 的分组卷积，这个操作将GFLOPs从4.4降到了2.4，但是它也将准确率从79.5%降到了78.3%。为了弥补准确率的下降，它将ResNet-50的基础通道数从64增加至96。这个操作将GFLOPs增加到了5.3，但是准确率提升到了80.5%。"><meta property="og:type" content="article"><meta property="og:url" content="https://Yangliuly1.github.io/202201-convnext/"><meta property="og:image" content="https://Yangliuly1.github.io/images/avatar.png"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Yangliuly1.github.io/images/avatar.png"><meta name=twitter:title content><meta name=twitter:description content="先导知识 Transformer 残差网络 Swin Transformer ResNeXt MobileNet v2 Layer Normalization 前言FAIR于近日提出的ConvNeXt[1]得到了广泛的关注，它引起关注的原因是它靠卷积结构便达到了ImageNet Top-1的准确率，与这两年来流行的使用Transformer[2]解决视觉问题的趋势背道而驰，让不少读者感叹“爷青回！”。再加上有何恺明，RBG，Yann LeCun的站台，很难不让人关注这篇文章。
ConvNeXt并没有特别复杂或者创新的结构，它的每一个网络细节都是已经在不止一个网络中被采用。而就是靠这些边角料的互相配合，却也达到了ImageNet Top-1的准确率。它涉及这些边角料的动机也非常简单：Transformer或者Swin-Transformer [3]怎么做，我也对应的调整，效果好就保留。当然这些边角料的摸索也是需要大量的实验数据支撑的，是一个耗时耗力耗资源的过程。通过对ConvNeXt的学习，我等调参侠不仅可以学习到诸多的炼丹经验，还可以一探其背后原理，在CNN和Transformer的大战中以内行的角度吃瓜。
论文：https://arxiv.org/abs/2201.03545
源码：https://github.com/facebookresearch/ConvNeXt
图1概括了ConvNeXt的所有优化点，它从ResNet-50[4]或者ResNet-200出发，依次从宏观设计，深度可分离卷积（ResNeXt[5]），逆瓶颈层（MobileNet v2[6]），大卷积核，细节设计这五个角度依次借鉴Swin Transformer的思想，然后在ImageNet-1K上进行训练和评估，最终得到ConvNeXt的核心结构。
图1：ConvNeXt的网络结构优化策略
1.1 训练方式随着深度学习在各个领域上的不断探索，残差网络锁采用的原始策略已经不能充分的压榨残差结构的性能，例如前不久的ResNet-timm[10] 就通过一系列训练trick将ResNet在ImageNet-1k的Top-1的准确率提升到80%+。在介绍ConvNeXt的结构优化之前，我们先简单介绍一下ConvNeXt的训练方法。我们知道ResNet-50在ImageNet-1k上最终的Top-1的准确率是76.1%，而图1中ResNet-50说的是78.8%，而这2.7%的准确率的提升正是依靠对训练策略的优化所达到的。
在ConvNeXt中，它的优化策略借鉴了Swin-Transformer。具体的优化策略包括：（1）将训练Epoch数从90增加到300；（2）优化器从SGD改为AdamW；（3）更复杂的数据扩充策略，包括Mixup，CutMix，RandAugment，Random Erasing等；（4）增加正则策略，例如随机深度[7]，标签平滑[8]，EMA[9]等。更具体的预训练和微调的超参数如图2。
图2：ConvNeXt的训练参数
1.2 宏观设计 1.2.1 每个阶段的计算占比（Stage Ratio）VGG提出了把骨干网络分成若干个网络块的结构，每个网络块通过池化操作将Feature Map降采样到不同的尺寸。在VGG中，每个网络块的网络层的数量基本是相同，但在之后的很多工作中，他们指出当深层的网络块层数更多时，模型的表现更好。例如，ResNet-50中共有4个不同的网络块，每个网络块又有若干个不同的基础层，一般是由卷积，BN等操作组成，它的每个网络块的层数是 。
在Swin-Transformer中，每个骨干网络被分成了4个不同的Stage，每个Stage又是由若干个Block组成，在Swin-Transformer中，这个Block的比例是 ，而对于更大的模型来说，这个比例是 。ConvNeXt的改进是将ResNet-50的每个Stage的block的比例调整到 ，最终得到的block数是 。从图1中可以看出这个改进将ResNet-50的准确率从78.8%提升至79.4%。注意这里模型的GFLOPs从4.1增加至4.5，这0.6%的准确率的提升也有一部分功劳要归功于模型参数量的增加。此外ConvNeXt还提供了更大的block数为 的模型。
1.2.2 Patchify Stem对于ImageNet数据集，我们通常采用 的输入尺寸，这个尺寸对于ViT等基于Transformer的模型来说是非常大的，它们通常使用一个步长为4，大小也为4的卷积将其尺寸降采样到 。因为这个卷积的步长和大小是完全相同的，所以它又是一个无覆盖的卷积，或者叫Patchify（补丁化)的卷积。这一部分在Swin-Transformer中叫做stem层，它是位于输入之后的一个降采样层。
在ConvNeXt中，Stem层也是一个步长为4，大小也为4的卷积操作，这一操作将准确率从79.4%提升至79.5%，GFLOPs从4.5降到4.4%。也有人指出使用覆盖的卷积（例如步长为4，卷积核大小为7的卷积）能够获得更好的表现。
1 2 3 4 stem = nn.Sequential( nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4), LayerNorm(dims[0], eps=1e-6, data_format=&#34;channels_first&#34;) ) 1.2.3 分组卷积在轻量级模型中，我们介绍了深度可分离卷积，它们将 卷积以通道为单位进行运算（深度卷积），然后再通过 卷积进行通道融合（点卷积）。而ResNeXt是一个更折中的方案，它通过分组卷积（将通道分组，然后以组为单位进行卷积）的方式来提升模型的计算速度。与之类似的是，Swin-Tranformer的Self-Attention也是以通道为单位的运算单元，不同的是可分离卷积是可学习的卷积核，Self-Attention是根据数据动态计算的权值。
在ConvNeXt中，也引入了分组卷积的思想。它将 卷积替换成了 的分组卷积，这个操作将GFLOPs从4.4降到了2.4，但是它也将准确率从79.5%降到了78.3%。为了弥补准确率的下降，它将ResNet-50的基础通道数从64增加至96。这个操作将GFLOPs增加到了5.3，但是准确率提升到了80.5%。"><meta name=application-name content="Yang"><meta name=apple-mobile-web-app-title content="Yang"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://Yangliuly1.github.io/202201-convnext/><link rel=next href=https://Yangliuly1.github.io/201512-resnet-v1/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/Yangliuly1.github.io\/202201-convnext\/"},"genre":"posts","wordcount":666,"url":"https:\/\/Yangliuly1.github.io\/202201-convnext\/","publisher":{"@type":"Organization","name":"Yangliuly1"},"author":{"@type":"Person","name":"Yangliuly1"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark")}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i> </a><span class="menu-item delimiter"></span><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i></a><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#先导知识>先导知识</a></li><li><a href=#前言>前言</a><ul><li><a href=#11-训练方式>1.1 训练方式</a></li><li><a href=#12-宏观设计>1.2 宏观设计</a></li><li><a href=#121-每个阶段的计算占比stage-ratio>1.2.1 每个阶段的计算占比（Stage Ratio）</a></li><li><a href=#122-patchify-stem>1.2.2 Patchify Stem</a></li><li><a href=#123-分组卷积>1.2.3 分组卷积</a></li><li><a href=#123-逆瓶颈层>1.2.3 逆瓶颈层</a></li><li><a href=#14-细节优化>1.4 细节优化</a></li><li><a href=#141-relu替换为gelu>1.4.1 ReLU替换为GELU</a></li><li><a href=#142-更少的激活函数>1.4.2 更少的激活函数</a></li><li><a href=#143-更少的归一化层>1.4.3 更少的归一化层</a></li><li><a href=#144-bn替换为ln>1.4.4 BN替换为LN</a></li><li><a href=#145-拆分降采样层>1.4.5 拆分降采样层</a></li></ul></li><li><a href=#2-convnext的多种版本>2. ConvNeXt的多种版本</a></li><li><a href=#3-总结>3. 总结</a></li><li><a href=#reference>Reference</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX"></h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=/about/ title=Author rel=author class=author>Yangliuly1</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=0001-01-01>0001-01-01</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=0001-01-01>0001-01-01</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;666 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;4 minutes&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#先导知识>先导知识</a></li><li><a href=#前言>前言</a><ul><li><a href=#11-训练方式>1.1 训练方式</a></li><li><a href=#12-宏观设计>1.2 宏观设计</a></li><li><a href=#121-每个阶段的计算占比stage-ratio>1.2.1 每个阶段的计算占比（Stage Ratio）</a></li><li><a href=#122-patchify-stem>1.2.2 Patchify Stem</a></li><li><a href=#123-分组卷积>1.2.3 分组卷积</a></li><li><a href=#123-逆瓶颈层>1.2.3 逆瓶颈层</a></li><li><a href=#14-细节优化>1.4 细节优化</a></li><li><a href=#141-relu替换为gelu>1.4.1 ReLU替换为GELU</a></li><li><a href=#142-更少的激活函数>1.4.2 更少的激活函数</a></li><li><a href=#143-更少的归一化层>1.4.3 更少的归一化层</a></li><li><a href=#144-bn替换为ln>1.4.4 BN替换为LN</a></li><li><a href=#145-拆分降采样层>1.4.5 拆分降采样层</a></li></ul></li><li><a href=#2-convnext的多种版本>2. ConvNeXt的多种版本</a></li><li><a href=#3-总结>3. 总结</a></li><li><a href=#reference>Reference</a></li></ul></nav></div></div><div class=content id=content><h2 id=先导知识 class=headerLink><a href=#%e5%85%88%e5%af%bc%e7%9f%a5%e8%af%86 class=header-mark></a>先导知识</h2><ul><li><a href=https://zhuanlan.zhihu.com/p/48508221 target=_blank rel="noopener noreffer">Transformer</a></li><li><a href=https://zhuanlan.zhihu.com/p/42706477 target=_blank rel="noopener noreffer">残差网络</a></li><li><a href=https://zhuanlan.zhihu.com/p/361366090 target=_blank rel="noopener noreffer">Swin Transformer</a></li><li><a href=https://zhuanlan.zhihu.com/p/51075096 target=_blank rel="noopener noreffer">ResNeXt</a></li><li><a href=https://zhuanlan.zhihu.com/p/50045821 target=_blank rel="noopener noreffer">MobileNet v2</a></li><li><a href=https://zhuanlan.zhihu.com/p/54530247 target=_blank rel="noopener noreffer">Layer Normalization</a></li></ul><h2 id=前言 class=headerLink><a href=#%e5%89%8d%e8%a8%80 class=header-mark></a>前言</h2><p>FAIR于近日提出的ConvNeXt[1]得到了广泛的关注，它引起关注的原因是它靠卷积结构便达到了ImageNet Top-1的准确率，与这两年来流行的使用Transformer[2]解决视觉问题的趋势背道而驰，让不少读者感叹“爷青回！”。再加上有何恺明，RBG，Yann LeCun的站台，很难不让人关注这篇文章。</p><p>ConvNeXt并没有特别复杂或者创新的结构，它的每一个网络细节都是已经在不止一个网络中被采用。而就是靠这些边角料的互相配合，却也达到了ImageNet Top-1的准确率。它涉及这些边角料的动机也非常简单：Transformer或者Swin-Transformer [3]怎么做，我也对应的调整，效果好就保留。当然这些边角料的摸索也是需要大量的实验数据支撑的，是一个耗时耗力耗资源的过程。通过对ConvNeXt的学习，我等调参侠不仅可以学习到诸多的炼丹经验，还可以一探其背后原理，在CNN和Transformer的大战中以内行的角度吃瓜。</p><p>论文：<a href=https://arxiv.org/abs/2201.03545 target=_blank rel="noopener noreffer">https://arxiv.org/abs/2201.03545</a></p><p>源码：<a href=https://github.com/facebookresearch/ConvNeXt target=_blank rel="noopener noreffer">https://github.com/facebookresearch/ConvNeXt</a></p><p>图1概括了ConvNeXt的所有优化点，它从ResNet-50[4]或者ResNet-200出发，依次从宏观设计，深度可分离卷积（ResNeXt[5]），逆瓶颈层（MobileNet v2[6]），大卷积核，细节设计这五个角度依次借鉴Swin Transformer的思想，然后在ImageNet-1K上进行训练和评估，最终得到ConvNeXt的核心结构。</p><p><img class=lazyload data-src=https://pic3.zhimg.com/v2-e3a2302c7907345362550980b27760ee_b.jpg data-srcset="https://pic3.zhimg.com/v2-e3a2302c7907345362550980b27760ee_b.jpg, https://pic3.zhimg.com/v2-e3a2302c7907345362550980b27760ee_b.jpg 1.5x, https://pic3.zhimg.com/v2-e3a2302c7907345362550980b27760ee_b.jpg 2x" data-sizes=auto alt=https://pic3.zhimg.com/v2-e3a2302c7907345362550980b27760ee_b.jpg title=https://pic3.zhimg.com/v2-e3a2302c7907345362550980b27760ee_b.jpg></p><p>图1：ConvNeXt的网络结构优化策略</p><h3 id=11-训练方式 class=headerLink><a href=#11-%e8%ae%ad%e7%bb%83%e6%96%b9%e5%bc%8f class=header-mark></a>1.1 训练方式</h3><p>随着深度学习在各个领域上的不断探索，残差网络锁采用的原始策略已经不能充分的压榨残差结构的性能，例如前不久的ResNet-timm[10] 就通过一系列训练trick将ResNet在ImageNet-1k的Top-1的准确率提升到80%+。在介绍ConvNeXt的结构优化之前，我们先简单介绍一下ConvNeXt的训练方法。我们知道ResNet-50在ImageNet-1k上最终的Top-1的准确率是76.1%，而图1中ResNet-50说的是78.8%，而这2.7%的准确率的提升正是依靠对训练策略的优化所达到的。</p><p>在ConvNeXt中，它的优化策略借鉴了Swin-Transformer。具体的优化策略包括：（1）将训练Epoch数从90增加到300；（2）优化器从SGD改为AdamW；（3）更复杂的数据扩充策略，包括Mixup，CutMix，RandAugment，Random Erasing等；（4）增加正则策略，例如随机深度[7]，标签平滑[8]，EMA[9]等。更具体的预训练和微调的超参数如图2。</p><p><img class=lazyload data-src=https://pic1.zhimg.com/v2-a784f1446decaa04534eb233465bdeb8_b.jpg data-srcset="https://pic1.zhimg.com/v2-a784f1446decaa04534eb233465bdeb8_b.jpg, https://pic1.zhimg.com/v2-a784f1446decaa04534eb233465bdeb8_b.jpg 1.5x, https://pic1.zhimg.com/v2-a784f1446decaa04534eb233465bdeb8_b.jpg 2x" data-sizes=auto alt=https://pic1.zhimg.com/v2-a784f1446decaa04534eb233465bdeb8_b.jpg title=https://pic1.zhimg.com/v2-a784f1446decaa04534eb233465bdeb8_b.jpg></p><p>图2：ConvNeXt的训练参数</p><h3 id=12-宏观设计 class=headerLink><a href=#12-%e5%ae%8f%e8%a7%82%e8%ae%be%e8%ae%a1 class=header-mark></a>1.2 宏观设计</h3><h3 id=121-每个阶段的计算占比stage-ratio class=headerLink><a href=#121-%e6%af%8f%e4%b8%aa%e9%98%b6%e6%ae%b5%e7%9a%84%e8%ae%a1%e7%ae%97%e5%8d%a0%e6%af%94stage-ratio class=header-mark></a>1.2.1 每个阶段的计算占比（Stage Ratio）</h3><p>VGG提出了把骨干网络分成若干个网络块的结构，每个网络块通过池化操作将Feature Map降采样到不同的尺寸。在VGG中，每个网络块的网络层的数量基本是相同，但在之后的很多工作中，他们指出当深层的网络块层数更多时，模型的表现更好。例如，ResNet-50中共有4个不同的网络块，每个网络块又有若干个不同的基础层，一般是由卷积，BN等操作组成，它的每个网络块的层数是 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=%283%2C4%2C6%2C3%29" data-srcset="https://www.zhihu.com/equation?tex=%283%2C4%2C6%2C3%29, https://www.zhihu.com/equation?tex=%283%2C4%2C6%2C3%29 1.5x, https://www.zhihu.com/equation?tex=%283%2C4%2C6%2C3%29 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=%283%2C4%2C6%2C3%29" title=[公式]> 。</p><p>在Swin-Transformer中，每个骨干网络被分成了4个不同的Stage，每个Stage又是由若干个Block组成，在Swin-Transformer中，这个Block的比例是 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=1%3A1%3A3%3A1" data-srcset="https://www.zhihu.com/equation?tex=1%3A1%3A3%3A1, https://www.zhihu.com/equation?tex=1%3A1%3A3%3A1 1.5x, https://www.zhihu.com/equation?tex=1%3A1%3A3%3A1 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=1%3A1%3A3%3A1" title=[公式]> ，而对于更大的模型来说，这个比例是 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=1%3A1%3A9%3A1" data-srcset="https://www.zhihu.com/equation?tex=1%3A1%3A9%3A1, https://www.zhihu.com/equation?tex=1%3A1%3A9%3A1 1.5x, https://www.zhihu.com/equation?tex=1%3A1%3A9%3A1 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=1%3A1%3A9%3A1" title=[公式]> 。ConvNeXt的改进是将ResNet-50的每个Stage的block的比例调整到 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=1%3A1%3A3%3A1" data-srcset="https://www.zhihu.com/equation?tex=1%3A1%3A3%3A1, https://www.zhihu.com/equation?tex=1%3A1%3A3%3A1 1.5x, https://www.zhihu.com/equation?tex=1%3A1%3A3%3A1 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=1%3A1%3A3%3A1" title=[公式]> ，最终得到的block数是 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=%283%2C3%2C9%2C3%29" data-srcset="https://www.zhihu.com/equation?tex=%283%2C3%2C9%2C3%29, https://www.zhihu.com/equation?tex=%283%2C3%2C9%2C3%29 1.5x, https://www.zhihu.com/equation?tex=%283%2C3%2C9%2C3%29 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=%283%2C3%2C9%2C3%29" title=[公式]> 。从图1中可以看出这个改进将ResNet-50的准确率从78.8%提升至79.4%。注意这里模型的GFLOPs从4.1增加至4.5，这0.6%的准确率的提升也有一部分功劳要归功于模型参数量的增加。此外ConvNeXt还提供了更大的block数为 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=%283%2C3%2C27%2C3%29" data-srcset="https://www.zhihu.com/equation?tex=%283%2C3%2C27%2C3%29, https://www.zhihu.com/equation?tex=%283%2C3%2C27%2C3%29 1.5x, https://www.zhihu.com/equation?tex=%283%2C3%2C27%2C3%29 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=%283%2C3%2C27%2C3%29" title=[公式]> 的模型。</p><h3 id=122-patchify-stem class=headerLink><a href=#122-patchify-stem class=header-mark></a>1.2.2 Patchify Stem</h3><p>对于ImageNet数据集，我们通常采用 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=224%5Ctimes224" data-srcset="https://www.zhihu.com/equation?tex=224%5Ctimes224, https://www.zhihu.com/equation?tex=224%5Ctimes224 1.5x, https://www.zhihu.com/equation?tex=224%5Ctimes224 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=224%5Ctimes224" title=[公式]> 的输入尺寸，这个尺寸对于ViT等基于Transformer的模型来说是非常大的，它们通常使用一个步长为4，大小也为4的卷积将其尺寸降采样到 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=56%5Ctimes56" data-srcset="https://www.zhihu.com/equation?tex=56%5Ctimes56, https://www.zhihu.com/equation?tex=56%5Ctimes56 1.5x, https://www.zhihu.com/equation?tex=56%5Ctimes56 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=56%5Ctimes56" title=[公式]> 。因为这个卷积的步长和大小是完全相同的，所以它又是一个无覆盖的卷积，或者叫Patchify（补丁化)的卷积。这一部分在Swin-Transformer中叫做stem层，它是位于输入之后的一个降采样层。</p><p>在ConvNeXt中，Stem层也是一个步长为4，大小也为4的卷积操作，这一操作将准确率从79.4%提升至79.5%，GFLOPs从4.5降到4.4%。也有人指出使用覆盖的卷积（例如步长为4，卷积核大小为7的卷积）能够获得更好的表现。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>stem = nn.Sequential(
</span></span><span class=line><span class=cl>    nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),
</span></span><span class=line><span class=cl>    LayerNorm(dims[0], eps=1e-6, data_format=&#34;channels_first&#34;)
</span></span><span class=line><span class=cl>)
</span></span></code></pre></td></tr></table></div></div><h3 id=123-分组卷积 class=headerLink><a href=#123-%e5%88%86%e7%bb%84%e5%8d%b7%e7%a7%af class=header-mark></a>1.2.3 分组卷积</h3><p>在轻量级模型中，我们介绍了深度可分离卷积，它们将 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=3+%5Ctimes+3" data-srcset="https://www.zhihu.com/equation?tex=3+%5Ctimes+3, https://www.zhihu.com/equation?tex=3+%5Ctimes+3 1.5x, https://www.zhihu.com/equation?tex=3+%5Ctimes+3 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=3+%5Ctimes+3" title=[公式]> 卷积以通道为单位进行运算（深度卷积），然后再通过 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=1%5Ctimes1" data-srcset="https://www.zhihu.com/equation?tex=1%5Ctimes1, https://www.zhihu.com/equation?tex=1%5Ctimes1 1.5x, https://www.zhihu.com/equation?tex=1%5Ctimes1 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=1%5Ctimes1" title=[公式]> 卷积进行通道融合（点卷积）。而ResNeXt是一个更折中的方案，它通过分组卷积（将通道分组，然后以组为单位进行卷积）的方式来提升模型的计算速度。与之类似的是，Swin-Tranformer的Self-Attention也是以通道为单位的运算单元，不同的是可分离卷积是可学习的卷积核，Self-Attention是根据数据动态计算的权值。</p><p>在ConvNeXt中，也引入了分组卷积的思想。它将 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=3%5Ctimes3" data-srcset="https://www.zhihu.com/equation?tex=3%5Ctimes3, https://www.zhihu.com/equation?tex=3%5Ctimes3 1.5x, https://www.zhihu.com/equation?tex=3%5Ctimes3 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=3%5Ctimes3" title=[公式]> 卷积替换成了 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=3%5Ctimes3" data-srcset="https://www.zhihu.com/equation?tex=3%5Ctimes3, https://www.zhihu.com/equation?tex=3%5Ctimes3 1.5x, https://www.zhihu.com/equation?tex=3%5Ctimes3 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=3%5Ctimes3" title=[公式]> 的分组卷积，这个操作将GFLOPs从4.4降到了2.4，但是它也将准确率从79.5%降到了78.3%。为了弥补准确率的下降，它将ResNet-50的基础通道数从64增加至96。这个操作将GFLOPs增加到了5.3，但是准确率提升到了80.5%。</p><h3 id=123-逆瓶颈层 class=headerLink><a href=#123-%e9%80%86%e7%93%b6%e9%a2%88%e5%b1%82 class=header-mark></a>1.2.3 逆瓶颈层</h3><p>瓶颈层是一个中间小，两头大的结构，最早在残差网络中被使用。而在MobileNet v2中则使用了一个中间大，两头小的结构，他们认为这个结构能够有效的避免信息流失。之后的 Transformer本质也是一个逆瓶颈层的架构，在Transformer中，Self-Attention的隐层节点数是512，而中间的全连接的隐层节点数是2048。这里的ConvNeXt也是使用了逆瓶颈层的结构，如图3.(a)和图3.(b)所示。这个策略将准确率提升至85.6%，GFLOPs降低至4.6。</p><p>在后续的工作中，作者尝试了使用更大的卷积核。为了适应更大的卷积核，作者将卷积的图3.(b)的深度卷积向上移动了一层，如图3.(c)。此操作将准确率降低到了79.9%，但是GFLOPS也下降到了4.1%。</p><p><img class=lazyload data-src=https://pic1.zhimg.com/v2-38c32788dc19eaac0e318b73a50df088_b.jpg data-srcset="https://pic1.zhimg.com/v2-38c32788dc19eaac0e318b73a50df088_b.jpg, https://pic1.zhimg.com/v2-38c32788dc19eaac0e318b73a50df088_b.jpg 1.5x, https://pic1.zhimg.com/v2-38c32788dc19eaac0e318b73a50df088_b.jpg 2x" data-sizes=auto alt=https://pic1.zhimg.com/v2-38c32788dc19eaac0e318b73a50df088_b.jpg title=https://pic1.zhimg.com/v2-38c32788dc19eaac0e318b73a50df088_b.jpg></p><p>图3. (a) ResNeXt的瓶颈层架构，(b) ConvNeXt的逆瓶颈层架构，(c)ConvNeXt又将可分离卷积向上移动了一层</p><p>1.3 更大卷积核</p><p>大卷积核是一个比较古老的卷积运算参数，上次看见这些大卷积核还是在AlexNet，VGG等这些上古网络中。当时使用小卷积核替代大卷积核，一个重要的原因是一个大卷积核（ <img class=lazyload data-src="https://www.zhihu.com/equation?tex=5%5Ctimes5" data-srcset="https://www.zhihu.com/equation?tex=5%5Ctimes5, https://www.zhihu.com/equation?tex=5%5Ctimes5 1.5x, https://www.zhihu.com/equation?tex=5%5Ctimes5 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=5%5Ctimes5" title=[公式]> 卷积）和两层的小卷积核（ <img class=lazyload data-src="https://www.zhihu.com/equation?tex=3%5Ctimes3" data-srcset="https://www.zhihu.com/equation?tex=3%5Ctimes3, https://www.zhihu.com/equation?tex=3%5Ctimes3 1.5x, https://www.zhihu.com/equation?tex=3%5Ctimes3 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=3%5Ctimes3" title=[公式]> 卷积）在拥有相同感受野的情况下，表现是比两层小卷积核略差的。在Swin-Transformer中，它们使用的是 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=7%5Ctimes7" data-srcset="https://www.zhihu.com/equation?tex=7%5Ctimes7, https://www.zhihu.com/equation?tex=7%5Ctimes7 1.5x, https://www.zhihu.com/equation?tex=7%5Ctimes7 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=7%5Ctimes7" title=[公式]> 自注意力窗口本质上也是一个大的计算窗口，因此这里作者也对大的卷积核重新进行了对照实验。</p><p>在ConvNeXt的实验中，它们尝试了 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=5%5Ctimes5" data-srcset="https://www.zhihu.com/equation?tex=5%5Ctimes5, https://www.zhihu.com/equation?tex=5%5Ctimes5 1.5x, https://www.zhihu.com/equation?tex=5%5Ctimes5 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=5%5Ctimes5" title=[公式]> ， <img class=lazyload data-src="https://www.zhihu.com/equation?tex=7%5Ctimes7" data-srcset="https://www.zhihu.com/equation?tex=7%5Ctimes7, https://www.zhihu.com/equation?tex=7%5Ctimes7 1.5x, https://www.zhihu.com/equation?tex=7%5Ctimes7 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=7%5Ctimes7" title=[公式]> ， <img class=lazyload data-src="https://www.zhihu.com/equation?tex=9%5Ctimes9" data-srcset="https://www.zhihu.com/equation?tex=9%5Ctimes9, https://www.zhihu.com/equation?tex=9%5Ctimes9 1.5x, https://www.zhihu.com/equation?tex=9%5Ctimes9 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=9%5Ctimes9" title=[公式]> 和 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=11%5Ctimes11" data-srcset="https://www.zhihu.com/equation?tex=11%5Ctimes11, https://www.zhihu.com/equation?tex=11%5Ctimes11 1.5x, https://www.zhihu.com/equation?tex=11%5Ctimes11 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=11%5Ctimes11" title=[公式]> 共4个不同尺寸的卷积操作。从图1的实验结果来看， <img class=lazyload data-src="https://www.zhihu.com/equation?tex=7%5Ctimes7" data-srcset="https://www.zhihu.com/equation?tex=7%5Ctimes7, https://www.zhihu.com/equation?tex=7%5Ctimes7 1.5x, https://www.zhihu.com/equation?tex=7%5Ctimes7 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=7%5Ctimes7" title=[公式]> 卷积操作的效果最好。它将模型的准确率提升至80.6%，GFLOPs些许增加至4.2。并且从这个实验结果中可以看出，大卷积核对模型速度的影响并不很大，这可能是得益于当前深度学习框架以及硬件对大卷积核的支持更友好。</p><h3 id=14-细节优化 class=headerLink><a href=#14-%e7%bb%86%e8%8a%82%e4%bc%98%e5%8c%96 class=header-mark></a>1.4 细节优化</h3><h3 id=141-relu替换为gelu class=headerLink><a href=#141-relu%e6%9b%bf%e6%8d%a2%e4%b8%bagelu class=header-mark></a>1.4.1 ReLU替换为GELU</h3><p>ReLU是比较早期的激活函数，近年来更多的模型选择使用GELU[11]作为激活函数，例如ConvNeXt要对齐的Swin Transformer。在ConvNeXt的实验中，GELU并没有提升模型的准确率和效率。但是为了对齐其它指标，ConvNeXt还是选择了GELU作为激活函数。</p><h3 id=142-更少的激活函数 class=headerLink><a href=#142-%e6%9b%b4%e5%b0%91%e7%9a%84%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0 class=header-mark></a>1.4.2 更少的激活函数</h3><p>在以往的卷积网络中，我们倾向于为每一个卷积操作都添加一个激活函数，但Transformer使用了更少的激活函数，Transformer是由一个自注意力层和两个MLP组成，但它仅在一个MLP上使用了激活函数。</p><p>ConvNeXt也借鉴了Transformer的思想，它仅在两个 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=1%5Ctimes1" data-srcset="https://www.zhihu.com/equation?tex=1%5Ctimes1, https://www.zhihu.com/equation?tex=1%5Ctimes1 1.5x, https://www.zhihu.com/equation?tex=1%5Ctimes1 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=1%5Ctimes1" title=[公式]> 卷积之间添加了一个GELU激活函数。实验结果表明这个操作将准确率从80.6%提升至81.3%。</p><h3 id=143-更少的归一化层 class=headerLink><a href=#143-%e6%9b%b4%e5%b0%91%e7%9a%84%e5%bd%92%e4%b8%80%e5%8c%96%e5%b1%82 class=header-mark></a>1.4.3 更少的归一化层</h3><p>Transformer也是一个归一化层使用的非常少的网络结构，因此在ConvNeXt中也使用了更少的归一化操作，它仅在第一个 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=1%5Ctimes1" data-srcset="https://www.zhihu.com/equation?tex=1%5Ctimes1, https://www.zhihu.com/equation?tex=1%5Ctimes1 1.5x, https://www.zhihu.com/equation?tex=1%5Ctimes1 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=1%5Ctimes1" title=[公式]> 卷积之前添加了一个BN，而更多的归一化操作对模型效果提升并没有帮助。通过这个操作将模型的准确率提升至81.4%。</p><h3 id=144-bn替换为ln class=headerLink><a href=#144-bn%e6%9b%bf%e6%8d%a2%e4%b8%baln class=header-mark></a>1.4.4 BN替换为LN</h3><p>根据我们之前的经验，BN[12]经常被用在CNN中，而LN[13]通常是用来解决BN在样本量过少的时候归一化统计量偏差过大的问题的。也有实验结果表明，如果将残差网络中的BN直接替换为LN的话，模型的性能反而会下降。</p><p>但是在ConvNeXt中，因为之前作者做了若干个将卷积网络向Transformer的改进，因此这里也尝试了将ConvNeXt中的BN替换为LN，令人意外的是，LN在ConvNeXt中要比BN表现的好，它将模型的准确率提升至81.5%。</p><p>因为上述改进都是非常小的点的优化，对模型的速度影响并不大，因此ConvNeXt的GFLOPs维持在了4.2没变。上述的所有改进如下面代码片段所示。注意下面的代码中的 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=1%5Ctimes1" data-srcset="https://www.zhihu.com/equation?tex=1%5Ctimes1, https://www.zhihu.com/equation?tex=1%5Ctimes1 1.5x, https://www.zhihu.com/equation?tex=1%5Ctimes1 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=1%5Ctimes1" title=[公式]> 卷积使用的是线性层来实现的。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>class Block(nn.Module):
</span></span><span class=line><span class=cl>    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):
</span></span><span class=line><span class=cl>        super().__init__()
</span></span><span class=line><span class=cl>        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv
</span></span><span class=line><span class=cl>        self.norm = LayerNorm(dim, eps=1e-6)
</span></span><span class=line><span class=cl>        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers
</span></span><span class=line><span class=cl>        self.act = nn.GELU()
</span></span><span class=line><span class=cl>        self.pwconv2 = nn.Linear(4 * dim, dim)
</span></span><span class=line><span class=cl>        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True) if layer_scale_init_value &gt; 0 else None
</span></span><span class=line><span class=cl>        self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()
</span></span><span class=line><span class=cl>    def forward(self, x):
</span></span><span class=line><span class=cl>        input = x
</span></span><span class=line><span class=cl>        x = self.dwconv(x)
</span></span><span class=line><span class=cl>        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -&gt; (N, H, W, C)
</span></span><span class=line><span class=cl>        x = self.norm(x)
</span></span><span class=line><span class=cl>        x = self.pwconv1(x)
</span></span><span class=line><span class=cl>        x = self.act(x)
</span></span><span class=line><span class=cl>        x = self.pwconv2(x)
</span></span><span class=line><span class=cl>        if self.gamma is not None:
</span></span><span class=line><span class=cl>            x = self.gamma * x
</span></span><span class=line><span class=cl>        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -&gt; (N, C, H, W)
</span></span><span class=line><span class=cl>        x = input + self.drop_path(x)
</span></span><span class=line><span class=cl>        return x
</span></span></code></pre></td></tr></table></div></div><h3 id=145-拆分降采样层 class=headerLink><a href=#145-%e6%8b%86%e5%88%86%e9%99%8d%e9%87%87%e6%a0%b7%e5%b1%82 class=header-mark></a>1.4.5 拆分降采样层</h3><p>在残差网络中，它通常使用的是步长为 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=2" data-srcset="https://www.zhihu.com/equation?tex=2, https://www.zhihu.com/equation?tex=2 1.5x, https://www.zhihu.com/equation?tex=2 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=2" title=[公式]> 的 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=3%5Ctimes3" data-srcset="https://www.zhihu.com/equation?tex=3%5Ctimes3, https://www.zhihu.com/equation?tex=3%5Ctimes3 1.5x, https://www.zhihu.com/equation?tex=3%5Ctimes3 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=3%5Ctimes3" title=[公式]> 卷积或者 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=1%5Ctimes1" data-srcset="https://www.zhihu.com/equation?tex=1%5Ctimes1, https://www.zhihu.com/equation?tex=1%5Ctimes1 1.5x, https://www.zhihu.com/equation?tex=1%5Ctimes1 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=1%5Ctimes1" title=[公式]> 卷积来进行降采样，这使得降采样层和其它层保持了基本相同的计算策略。但是Swin Transformer将降采样层从其它运算中剥离开来，即使用一个步长为2的 <img class=lazyload data-src="https://www.zhihu.com/equation?tex=2%5Ctimes2" data-srcset="https://www.zhihu.com/equation?tex=2%5Ctimes2, https://www.zhihu.com/equation?tex=2%5Ctimes2 1.5x, https://www.zhihu.com/equation?tex=2%5Ctimes2 2x" data-sizes=auto alt="https://www.zhihu.com/equation?tex=2%5Ctimes2" title=[公式]> 卷积插入到不同的Stage之间。ConvNeXt也是采用了这个策略，而且在降采样前后各加入了一个LN，而且在全局均值池化之后也加入了一个LN，这些归一化用来保持模型的稳定性。这个策略将模型的准确率提升至82.0% 。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>downsample_layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>()</span> 
</span></span><span class=line><span class=cl><span class=c1># stem也可以看成下采样层，一起存到downsample_layers中，推理时通过index进行访问</span>
</span></span><span class=line><span class=cl><span class=n>stem</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>in_chans</span><span class=p>,</span> <span class=n>dims</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>4</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>LayerNorm</span><span class=p>(</span><span class=n>dims</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-6</span><span class=p>,</span> <span class=n>data_format</span><span class=o>=</span><span class=s2>&#34;channels_first&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>downsample_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>stem</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>downsample_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>LayerNorm</span><span class=p>(</span><span class=n>dims</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-6</span><span class=p>,</span> <span class=n>data_format</span><span class=o>=</span><span class=s2>&#34;channels_first&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>dims</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>dims</span><span class=p>[</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>],</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>downsample_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>downsample_layer</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>最终，ConvNeXt的结构如图4.(b)所示，与之对应的是图4.(a)的残差网络。</p><p><img class=lazyload data-src=https://pic2.zhimg.com/v2-901ff3b7ae397a58e93a4131b5ed4cd9_b.jpg data-srcset="https://pic2.zhimg.com/v2-901ff3b7ae397a58e93a4131b5ed4cd9_b.jpg, https://pic2.zhimg.com/v2-901ff3b7ae397a58e93a4131b5ed4cd9_b.jpg 1.5x, https://pic2.zhimg.com/v2-901ff3b7ae397a58e93a4131b5ed4cd9_b.jpg 2x" data-sizes=auto alt=https://pic2.zhimg.com/v2-901ff3b7ae397a58e93a4131b5ed4cd9_b.jpg title=https://pic2.zhimg.com/v2-901ff3b7ae397a58e93a4131b5ed4cd9_b.jpg></p><p>图4：（a）残差网络和（b）ConvNeXt</p><h2 id=2-convnext的多种版本 class=headerLink><a href=#2-convnext%e7%9a%84%e5%a4%9a%e7%a7%8d%e7%89%88%e6%9c%ac class=header-mark></a>2. ConvNeXt的多种版本</h2><p>ConvNeXt提供了多个参数尺度的模型，他们的参数结构和在ImageNet-1K的Top-1的准确率如表1。</p><table><thead><tr><th>网络结构</th><th>输入尺寸</th><th>通道数</th><th>Block数</th><th>准确率</th></tr></thead><tbody><tr><td>ConvNeXt-T</td><td>224</td><td>96, 192, 384, 768</td><td>3, 3, 9, 3</td><td>82.1%</td></tr><tr><td>ConvNeXt-S</td><td>224</td><td>96, 192, 384, 768</td><td>3, 3, 27, 3</td><td>83.1%</td></tr><tr><td>ConvNeXt-B</td><td>384</td><td>128, 256, 512, 1024</td><td>3, 3, 27, 3</td><td>85.1%</td></tr><tr><td>ConvNeXt-L</td><td>384</td><td>192, 384, 768, 1536</td><td>3, 3, 27, 3</td><td>85.5%</td></tr><tr><td>ConvNeXt-XL</td><td>384</td><td>256, 512, 1024, 2048</td><td>3, 3, 27, 3</td><td>87.8%</td></tr></tbody></table><p>表1：不同尺度的ConvNeXt的超参数值以及准确率</p><p>除了这里介绍的ConvNeXt，论文中还设计了一个和ViT[14]结构类似的Isotropic ConvNeXt，即采用同质结构的ConvNeXt：即先通过一个patch embedding层得到输入patch的embedding，然后送入参数共享的网络中进行特征学习。从效果上来看，Isotropic的效果和ViT基本一样，这说明了Transformer在视觉任务上并没有明显的结构优势。</p><h2 id=3-总结 class=headerLink><a href=#3-%e6%80%bb%e7%bb%93 class=header-mark></a>3. 总结</h2><p>将Transformer移植到计算机视觉领域是一个由结果推原理的一个计算流程，并没有什么资料说明Self-Attention是在原理上比卷积更适用于图像任务。我们也没必要过分神话Self-Attention，它本质上还是矩阵的密集计算，它的提出有很大的功劳是得益于计算性能的提升。在没有充分理论支持的情况下，自注意力注定只是计算机视觉方向的一个过客。但是在此时此刻，Transformer能在计算机视觉掀起多大的波澜，还是要看硬件性能的提升效率，新范式的提出时间，以及卷积的反抗。</p><p>这里介绍的ConvNeXt就是CNN的一个很好的反击，它在保持CNN结构的基础之上，通过“抄袭”Swin Transformer等方法的调参技巧，证明了Transformer在视觉领域上的突出表现并不是Transformer在理论上更适合图像数据，而只是近年来的诸多的提升准确率的小Trick带来的附加作用，就像前不久的ResNet-Timm[10]，它也能在仅仅修改调参技巧也能把残差网络的准确率提升到80%+。</p><p>我不想就此否定Transformer在视觉方向的突出贡献，它将Transformer在NLP领域的进展带来的经验技巧融入到CV领域，对CV领域的发展也是非常具有促进意义的，而且对目前的多模态深度学习的发展也非常重要。我们也期待将来有一天，我们能找到一个比CNN和Transformer都更适合于图像任务的结构范式。</p><h2 id=reference class=headerLink><a href=#reference class=header-mark></a>Reference</h2><p>[1] Liu, Zhuang, et al. &ldquo;A ConvNet for the 2020s.&rdquo; <em>arXiv preprint arXiv:2201.03545</em> (2022).</p><p>[2] Vaswani, Ashish, et al. &ldquo;Attention is all you need.&rdquo; <em>Advances in neural information processing systems</em>. 2017.</p><p>[3] Liu, Ze, et al. &ldquo;Swin transformer: Hierarchical vision transformer using shifted windows.&rdquo; <em>arXiv preprint arXiv:2103.14030</em> (2021).</p><p>[4] He, Kaiming, et al. &ldquo;Deep residual learning for image recognition.&rdquo; <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2016.</p><p>[5] Xie, Saining, et al. &ldquo;Aggregated residual transformations for deep neural networks.&rdquo; <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2017.</p><p>[6] Sandler, Mark, et al. &ldquo;Mobilenetv2: Inverted residuals and linear bottlenecks.&rdquo; <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2018.</p><p>[7] Huang, Gao, et al. &ldquo;Deep networks with stochastic depth.&rdquo; <em>European conference on computer vision</em>. Springer, Cham, 2016.</p><p>[8] Müller, Rafael, Simon Kornblith, and Geoffrey Hinton. &ldquo;When does label smoothing help?.&rdquo; <em>arXiv preprint arXiv:1906.02629</em> (2019).</p><p>[9] Polyak, Boris T., and Anatoli B. Juditsky. &ldquo;Acceleration of stochastic approximation by averaging.&rdquo; <em>SIAM journal on control and optimization</em> 30.4 (1992): 838-855.</p><p>[10] Wightman, Ross, Hugo Touvron, and Hervé Jégou. &ldquo;Resnet strikes back: An improved training procedure in timm.&rdquo; <em>arXiv preprint arXiv:2110.00476</em> (2021).</p><p>[11] Hendrycks, Dan, and Kevin Gimpel. &ldquo;Gaussian error linear units (gelus).&rdquo; <em>arXiv preprint arXiv:1606.08415</em> (2016).</p><p>[12] Ioffe, Sergey, and Christian Szegedy. &ldquo;Batch normalization: Accelerating deep network training by reducing internal covariate shift.&rdquo; <em>International conference on machine learning</em>. PMLR, 2015.</p><p>[13] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. &ldquo;Layer normalization.&rdquo; <em>arXiv preprint arXiv:1607.06450</em> (2016).</p><p>[14] Dosovitskiy, Alexey, et al. &ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.&rdquo; <em>arXiv preprint arXiv:2010.11929</em> (2020).</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 0001-01-01</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/201512-resnet-v1/ class=next rel=next title="Deep Residual Learning for Image Recognition">Deep Residual Learning for Image Recognition<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/about/ target=_blank rel="noopener noreferrer">Yangliuly1</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},data:{"desktop-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!","mobile-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},table:{sort:!0},typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},duration:-1,speed:100}}</script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/typeit/typeit.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript></div></body></html>