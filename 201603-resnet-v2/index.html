<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>Identity Mappings in Deep Residual Networks - Yang</title><meta name=Description content><meta property="og:title" content="Identity Mappings in Deep Residual Networks"><meta property="og:description" content="摘要：ResNet v2 恒等连接的支路详细讨论与实验。"><meta property="og:type" content="article"><meta property="og:url" content="https://Yangliuly1.github.io/201603-resnet-v2/"><meta property="og:image" content="https://Yangliuly1.github.io/images/avatar.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-24T09:04:11+08:00"><meta property="article:modified_time" content="2022-04-24T09:04:11+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Yangliuly1.github.io/images/avatar.png"><meta name=twitter:title content="Identity Mappings in Deep Residual Networks"><meta name=twitter:description content="摘要：ResNet v2 恒等连接的支路详细讨论与实验。"><meta name=application-name content="Yang"><meta name=apple-mobile-web-app-title content="Yang"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://Yangliuly1.github.io/201603-resnet-v2/><link rel=prev href=https://Yangliuly1.github.io/201512-resnet-v1/><link rel=next href=https://Yangliuly1.github.io/201611-resnext/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Identity Mappings in Deep Residual Networks","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/Yangliuly1.github.io\/201603-resnet-v2\/"},"genre":"posts","keywords":"图像分类","wordcount":275,"url":"https:\/\/Yangliuly1.github.io\/201603-resnet-v2\/","datePublished":"2022-04-24T09:04:11+08:00","dateModified":"2022-04-24T09:04:11+08:00","publisher":{"@type":"Organization","name":"Yangliuly1"},"author":{"@type":"Person","name":"Yangliuly1"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark")}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(t){const e=document.getElementsByTagName("meta");for(let n=0;n<e.length;n++)if(e[n].getAttribute("name")===t)return e[n];return''}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?setTheme("dark"):setTheme("light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?setTheme("dark"):setTheme("light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class="far fa-edit fa-fw"></i></span><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i> </a><span class="menu-item delimiter"></span><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=Yang><span class=header-title-pre><i class="far fa-edit fa-fw"></i></span><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/Yangliuly1 title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github fa-fw"></i></a><a href=# onclick=return!1 class="menu-item theme-select" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title="Switch Theme"><option value=light>Light</option><option value=dark>Dark</option><option value=black>Black</option><option value=auto>Auto</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1残差单元>1、残差单元</a></li><li><a href=#2恒等跳跃连接>2、恒等跳跃连接</a></li><li><a href=#3激活函数>3、激活函数</a></li></ul></li><li><a href=#训练与测试>训练与测试</a></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Identity Mappings in Deep Residual Networks</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=/about/ title=Author rel=author class=author>Yangliuly1</a>
</span>&nbsp;<span class=post-category>included in </span>&nbsp;<span class=post-category>category <a href=/categories/%E4%B8%93%E4%B8%9A%E8%AE%BA%E6%96%87/><i class="far fa-folder fa-fw"></i>专业论文</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-04-24>2022-04-24</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2022-04-24>2022-04-24</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;275 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;2 minutes&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#文献信息>文献信息</a></li><li><a href=#个人理解>个人理解</a></li><li><a href=#背景知识>背景知识</a></li><li><a href=#原理方法>原理方法</a><ul><li><a href=#1残差单元>1、残差单元</a></li><li><a href=#2恒等跳跃连接>2、恒等跳跃连接</a></li><li><a href=#3激活函数>3、激活函数</a></li></ul></li><li><a href=#训练与测试>训练与测试</a></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></div><div class=content id=content><p>摘要：ResNet v2 恒等连接的支路详细讨论与实验。</p><h1 id=identity-mappings-in-deep-residual-networks class=headerLink><a href=#identity-mappings-in-deep-residual-networks class=header-mark></a>Identity Mappings in Deep Residual Networks</h1><h2 id=文献信息 class=headerLink><a href=#%e6%96%87%e7%8c%ae%e4%bf%a1%e6%81%af class=header-mark></a>文献信息</h2><table><thead><tr><th>信息</th><th>内容</th></tr></thead><tbody><tr><td>日期</td><td>2016.03</td></tr><tr><td>作者</td><td><a href=http://kaiminghe.com/ target=_blank rel="noopener noreffer">Kaiming He</a> et al.</td></tr><tr><td>机构</td><td>Microsoft Research</td></tr><tr><td>来源</td><td>ECCV2016</td></tr><tr><td>链接</td><td><a href=https://arxiv.org/abs/1603.05027 target=_blank rel="noopener noreffer">Identity Mappings in Deep Residual Networks</a></td></tr><tr><td>代码</td><td><a href=https://github.com/KaimingHe target=_blank rel="noopener noreffer">KaimingHe</a>/<a href=https://github.com/KaimingHe/resnet-1k-layers target=_blank rel="noopener noreffer">resnet-1k-layers</a></td></tr></tbody></table><h2 id=个人理解 class=headerLink><a href=#%e4%b8%aa%e4%ba%ba%e7%90%86%e8%a7%a3 class=header-mark></a>个人理解</h2><blockquote><p><strong style=color:red>问题:</strong> 残差块结构分析讨论，是否影响网络收敛速度和泛化能力。；</p><p><strong style=color:red>方法:</strong> 分析残差网络基本构件（<code>residual building block</code>）中的信号传播，本文发现当使用恒等映射（identity mapping）作为快捷连接（skip connection）并且将激活函数移至加法操作后面时，前向-反向信号都可以在两个 block 之间直接传播而不受到任何变换操作的影响。<strong>同时大量实验结果证明了恒等映射的重要性</strong>。</p><p><strong style=color:red>结论:</strong> 网络更易于训练并且泛化性能提升；</p><p><strong style=color:red>理解:</strong></p><ul><li>证明恒等映射（<code>identity mapping</code>）作为快捷连接（<code>skip connection</code>）对于残差块的重要性。</li><li>残差单元将激活函数（先BN再ReLU）移到权值层之前，即<strong>预激活（pre-activation）</strong>，而<strong>后激活（post-activation）</strong>，并且预激活的单元中的所有权值层的输入都是归一化的信号，使得网络更易于训练并且泛化性能也得到提升。</li><li>注意：预激活模式可能仅仅适用于深层模型，如论文中实验的ResNet-110。</li></ul><p><strong style=color:red>优化：</strong>还有什么值得改进与优化的。</p></blockquote><hr><h2 id=背景知识 class=headerLink><a href=#%e8%83%8c%e6%99%af%e7%9f%a5%e8%af%86 class=header-mark></a>背景知识</h2><h2 id=原理方法 class=headerLink><a href=#%e5%8e%9f%e7%90%86%e6%96%b9%e6%b3%95 class=header-mark></a>原理方法</h2><h3 id=1残差单元 class=headerLink><a href=#1%e6%ae%8b%e5%b7%ae%e5%8d%95%e5%85%83 class=header-mark></a>1、残差单元</h3><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/v2-e3f649b10152fba79782fa0f511c1c6a_r.jpg width=75%></div><p>残差单元公式:
$$
y_l=h(x_l)+F(x_l,W_l) \
x_{l+1} = f(y_l)
$$
其中，$x_l$是第l个残差单元的输入特征，$W={W_{l,k|1≤k≤K}}$是一个与第 l个残差单元相关的权重和偏差的集合，K是残差单元内部的层的数量（K分别为2和3，即普通残差块和瓶颈残差块）。 $F$是残差函数，函数$f$是元素加和后的激活操作（ResNet-v1中采用的是ReLU），函数$h$是恒等映射$h(x_l)=x_l$。</p><p>如果$f$是一个恒等映射:$x_{l+1}\equiv x_{l}$，得：</p><p>$$
x_{l+1}=x_{l}+\mathcal{F}(x_{l},\mathcal{W}_{l}) \text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }
$$</p><p>递归地，$x_{l+2}=x_{l+1}+\mathcal{F}(x_{l+1},\mathcal{W}<em>{l+1})=x</em>{l}+\mathcal{F}(x_{l},\mathcal{W}<em>{l})+\mathcal{F}(x</em>{l+1},\mathcal{W}_{l+1})$,得：</p><p>$$
x_{L}=x_{l}+\sum_{i=1}^{L-1}\mathcal{F}(x_{i},\mathcal{W}_{i}) \text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }
$$</p><p>特性：</p><ol><li>任意深层单元的特征都可以由起始特征$x0$与先前所有残差函数相加得到，而普通通网络的深层特征是由一系列的矩阵向量相乘得到。<strong>残差网络是连加$x_{L}=x_{0}+\sum_{i=0}^{L-1}\mathcal{F}(x_{i},\mathcal{W}_{i})$，普通网络是连乘</strong>$\prod_{i=0}^{L-1}W_{i}{x}_0$（忽略ReLU和BN）。</li><li>反向传播特性，假设损失函数为E，<strong>残差结构表明梯度$\frac{\partial E}{\partial {{x}_{l}}}$可以被分解成两个部分进行反向传播</strong>：其中 $\frac{\partial E}{\partial {{x}<em>{L}}}$直接传递信息而不涉及任何权重层，而另一部分$\frac{\partial E}{\partial {{x}</em>{L}}}\left(\frac{\partial {\sum_{i=l}^{L-1}\mathcal{F}}}{\partial {{x}<em>{l}}}\right)$表示通过权重层传播的信息。$\frac{\partial E}{\partial {{x}</em>{L}}}$保证了信息能够直接传回任意浅单元l。等式同样表明了在一个mini-batch中梯度$\frac{\partial E}{\partial {{x}<em>{l}}}$不可能出现消失的情况，因为通常$\frac{\partial {\sum</em>{i=l}^{L-1}\mathcal{F}}}{\partial {{x}_{l}}}$对于一个mini-batch的全部样本不可能都为-1。这意味着，哪怕权重是任意小的，也不可能出现梯度消失的情况。</li></ol><p>$$
\frac{\partial E}{\partial {{x}<em>{l}}}=\frac{\partial E}{\partial {{x}</em>{L}}}\frac{\partial {{x}<em>{L}}}{\partial {{x}</em>{l}}}=\frac{\partial E}{\partial{{x}<em>{L}}}\left(1+\frac{\partial {\sum</em>{i=l}^{L-1}\mathcal{F}({x}<em>{i}, \mathcal{W}</em>{i})}}{\partial {{x}_{l}}}\right) \text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }
$$</p><h3 id=2恒等跳跃连接 class=headerLink><a href=#2%e6%81%92%e7%ad%89%e8%b7%b3%e8%b7%83%e8%bf%9e%e6%8e%a5 class=header-mark></a>2、恒等跳跃连接</h3><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/v2-a90049141bc4cc28ff871f920bae8124_r.jpg width=75%></div><p>验证公式$h$是一个恒等连接的重要性，假设$h(x_{l})=\lambda_{l} x_{l}$，$λ_l$ 是可调节的变量，假设 $f$是恒等映射，得：
$$
{x}<em>{L} = (\prod</em>{i=l}^{L-1}\lambda_{i}){x}<em>{l} + \sum</em>{i=l}^{L-1} (\prod_{j=i+1}^{L-1}\lambda_{\tiny j}) \mathcal{F}({x}<em>{i}, \mathcal{W}</em>{i}) \</p><p>{x}<em>{L} = (\prod</em>{i=l}^{L-1}\lambda_{i}){x}<em>{l} + \sum</em>{i=l}^{L-1}\mathcal{\hat{F}}({x}<em>{i}, \mathcal{W}</em>{i}) \text{ }
$$</p><p>$$
\frac{\partial E}{\partial {{x}<em>{l}}}=\frac{\partial E}{\partial {{x}</em>{L}}}\left((\prod_{i=l}^{L-1}\lambda_{i})+\frac{\partial {\sum_{i=l}^{L-1}\mathcal{\hat{F}}({x}<em>{i}\mathcal{W}</em>{i})}}{\partial {{x}_{l}}}\right)
$$</p><p>公式说明：</p><ol><li><p>第一项会额外引进一个缩放因子$\prod_{i=l}^{L-1}\lambda_{i}$。对于一个极深的网络(L非常大)，如果$λ_i>1$，这个系数将会指数级大；如果 $λi&lt;1$，这个系数将会变得指数级小并且消失，从而阻断从捷径反向传来的信号，并迫使它流向权重层，这将对优化造成困难。</p></li><li><p>原始的identity skip connection被一个简单的缩放（$h(x_{l})=\lambda_{l}x_{l}$）代替。如果skip connection $h(x_l)$表示更复杂的变换（例如gating或者1x1卷积），公式的第一项变成了$\prod_{i=l}^{L-1}h&rsquo;_{i}$，这里$h′$是$h$的导数。这个乘积也可能阻碍信息的反向传播并且阻碍训练训练过程。</p></li></ol><p><strong>实验对比</strong>：作者在CIFAR-10数据集上ResNet_v1-110实验。极深的ResNet_v1-110有54个两层残差单元(包含3x3卷积层)。假设f为恒等映射，这部分的实验中，和ResNet_v1一致，令f=ReLU。</p><p>分别对恒等跳跃连接支路（灰色箭头）：乘以一个常量缩放、sigmoid函数权重、1x1卷积、dropout，</p><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/v2-a90049141bc4cc28ff871f920bae8124_r.jpg width=75%></div><ul><li>常量缩放$\lambda$：常数比例相乘。</li></ul><p>对于所有的捷径连接，设置λ=0.5。进一步考虑$F$的两种缩放情况：(1) F不进行缩放。(2) $f$用一个常量1−λ=0.5进行缩放，类似highway gating，但gate是固定的。(1)收敛的不好（测试错误率高于20%），(2)能够收敛，但是测试错误率（12.35%）比原始的ResNet-110高很多。表明当shortcut信号scaled down时，优化有困难。</p><ul><li>exclusive gating：1x1-sigmod函数训练权重，与两支路相乘。</li></ul><p>和Highway Network一样，采用一个gating机制，作者考虑一个后面接sigmoid激活函数的gating函数$$g(x)=(W_gx+b_g)$$。在一个卷积网络中g(x)可以通过1x1卷积层实现。gating函数通过元素元素级别的乘法调节信号。</p><p>exclusive gating机制的影响是两面的，当1−g(x)接近1时，gated shortcut连接是十分接近于identity的，这有助于信息的传播；但是在这种情况下，g(x)接近0，并且抑制了函数FF。为了单独研究gating函数对于shortcut path的影响，我们接下来研究了没有exclusive gating机制。</p><ul><li>Shortcut-only gating：1x1-sigmod函数训练权重，与跳跃支路相乘。效果差</li></ul><p>在这种情况下，函数F不进行缩放；shortcut path只用1−g(x)进行缩放）。bg的初始值对于这种情况仍然很重要。当bg的初始值是0时（所以1-g(x)的初始化的期望值为0.5），网络收敛到一个很烂的结果。这也是由于训练误差很高。</p><p>当bg的初始值是一个非常小的负数（very negatively biased 例如 -6），1−g(x)的值非常接近1并且shortcut连接接近于identity映射。因此结果（6.69%表1）是很接近ResNet_v1-110。</p><ul><li><p>1x1 Convolutional shortcut：浅层好，深层差。</p></li><li><p>Dropout shortcut：未收敛到好结果。</p></li></ul><p>讨论：
如上图中灰色箭头所示，shortcut连接是信息传递最直接的路径。shortcut连接中的操作 (scale、gating、1××1 conv及 dropout) 会阻碍信息的传递，以致于优化困难。</p><p>值得注意的是1×1的卷积shortcut连接引入了更多的参数，本应该具有比恒等捷径连接更强大的表达能力。事实上，shortcut-only gating 和1×1的卷积涵盖了恒等捷径连接的解空间(即，他们能够以恒等捷径连接的形式进行优化)。然而，它们的训练误差比恒等捷径连接的训练误差要高得多，这表明了这些模型退化问题的原因是优化问题，而不是表达能力的问题</p><h3 id=3激活函数 class=headerLink><a href=#3%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0 class=header-mark></a>3、激活函数</h3><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/v2-df6cf78b3c0a90fd00a1d71884387498_r.jpg width=75%></div><p>先验证了$h$和$F$公式为恒等映射的重要性，再验证二者相加后的输出为恒等映射，公式$f$如果也是一个恒等连接的重要性，通过调节激活函数 (<code>ReLU and/or BN</code>) 的位置，来使$f$是恒等映射原始的残差单元输出。</p><p>实验对比：</p><div align=center><img src=https://cloud-resources-data.oss-cn-chengdu.aliyuncs.com/blog/v2-5e5cf9e16a23a8a629165482bcd3de47_r.jpg width=75%></div><ol><li>BN after addition 效果比基准差，BN 层移到相加操作后面会阻碍信号传播，一个明显的现象就是训练初期误差下降缓慢。</li><li>ReLU before addition 这样组合的话残差函数分支的输出就一直保持非负，这会影响到模型的表示能力，而实验结果也表明这种组合比基准差。</li><li>Post-activation or pre-activation 原来的设计中相加操作后面还有一个 ReLU 激活函数，这个激活函数会影响到残差单元的两个分支，现在将它移到残差函数分支上，快捷连接分支不再受到影响。</li></ol><p>注意：预激活方式又可以分为两种：只将 ReLU 放在前面，或者将 ReLU 和 BN都放到前面，根据实验结果可以看出 full pre-activation 的效果要更好。</p><p>预激活有两个方面的优点：1) [公式] 变为恒等映射，使得网络更易于优化；2)使用 BN 作为预激活可以加强对模型的正则化。</p><p>Ease of optimization 这在训练 1001 层残差网络时尤为明显。使用原来设计的网络在起始阶段误差下降很慢，因为 [公式] 是 ReLU 激活函数，当信号为负时会被截断，使模型无法很好地逼近期望函数；而使用预激活网络中的$f$是恒等映射，信号可以在不同单元直接直接传播。本文使用的 1001层网络优化速度很快，并且得到了最低的误差。</p><p>$f$为 ReLU 对浅层残差网络的影响并不大，如图 6-right 所示。本文认为是当网络经过一段时间的训练之后权值经过适当的调整，使得单元输出基本都是非负，此时$f$不再对信号进行截断。但是截断现象在超过 1000层的网络中经常发生。</p><p>Reducing overfitting，使用了预激活的网络的训练误差稍高，但却得到更低的测试误差，本文推测这是 BN 层的正则化效果所致。在原始残差单元中，尽管BN对信号进行了标准化，但是它很快就被合并到捷径连接(shortcut)上，组合的信号并不是被标准化的。这个非标准化的信号又被用作下一个权重层的输入。与之相反，本文的预激活（pre-activation）版本的模型中，权重层的输入总是标准化的。</p><h2 id=训练与测试 class=headerLink><a href=#%e8%ae%ad%e7%bb%83%e4%b8%8e%e6%b5%8b%e8%af%95 class=header-mark></a>训练与测试</h2><p>略。</p><h2 id=参考文献 class=headerLink><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae class=header-mark></a>参考文献</h2></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-04-24</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/>图像分类</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/201512-resnet-v1/ class=prev rel=prev title="Deep Residual Learning for Image Recognition"><i class="fas fa-angle-left fa-fw"></i>Deep Residual Learning for Image Recognition</a>
<a href=/201611-resnext/ class=next rel=next title="Aggregated Residual Transformations for Deep Neural Networks">Aggregated Residual Transformations for Deep Neural Networks<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/about/ target=_blank rel="noopener noreferrer">Yangliuly1</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},data:{"desktop-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!","mobile-header-typeit":"日落跌进迢迢星野,人间忽晚,山河已秋!"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},table:{sort:!0},typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},duration:-1,speed:100}}</script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/typeit/typeit.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript></div></body></html>